{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 11 17:21:43 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   36C    P0              57W / 500W |      0MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          Off | 00000000:0C:00.0 Off |                    0 |\n",
      "| N/A   36C    P0              52W / 500W |      0MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-80GB          Off | 00000000:0D:00.0 Off |                    0 |\n",
      "| N/A   34C    P0              56W / 500W |      0MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-80GB          Off | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   36C    P0              55W / 500W |      0MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.5.1 (from -r requirements.txt (line 1))\n",
      "  Downloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting torchao==0.6.1 (from -r requirements.txt (line 2))\n",
      "  Downloading torchao-0.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting torchvision==0.20.1 (from -r requirements.txt (line 3))\n",
      "  Downloading torchvision-0.20.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting torchtune==0.4.0 (from -r requirements.txt (line 4))\n",
      "  Downloading torchtune-0.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting wandb==0.19.0 (from -r requirements.txt (line 5))\n",
      "  Downloading wandb-0.19.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting python-dotenv==1.0.1 (from -r requirements.txt (line 6))\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting filelock (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (4.11.0)\n",
      "Collecting networkx (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (3.1.4)\n",
      "Collecting fsspec (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.1.0 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (75.1.0)\n",
      "Collecting sympy==1.13.1 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting numpy (from torchvision==0.20.1->-r requirements.txt (line 3))\n",
      "  Downloading numpy-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.20.1->-r requirements.txt (line 3))\n",
      "  Downloading pillow-11.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting datasets (from torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting huggingface-hub (from torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors (from torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting sentencepiece (from torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tiktoken (from torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading tiktoken-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting blobfile>=2 (from torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading blobfile-3.0.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torchtune==0.4.0->-r requirements.txt (line 4)) (4.66.5)\n",
      "Collecting omegaconf (from torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torchtune==0.4.0->-r requirements.txt (line 4)) (5.9.0)\n",
      "Collecting click!=8.0.0,>=7.1 (from wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from wandb==0.19.0->-r requirements.txt (line 5)) (3.10.0)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 (from wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting pydantic<3,>=2.6 (from wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading pydantic-2.10.3-py3-none-any.whl.metadata (172 kB)\n",
      "Requirement already satisfied: pyyaml in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from wandb==0.19.0->-r requirements.txt (line 5)) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from wandb==0.19.0->-r requirements.txt (line 5)) (2.32.3)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading sentry_sdk-2.19.2-py2.py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting setproctitle (from wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading setproctitle-1.3.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting pycryptodomex>=3.8 (from blobfile>=2->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from blobfile>=2->torchtune==0.4.0->-r requirements.txt (line 4)) (2.2.3)\n",
      "Collecting lxml>=4.9 (from blobfile>=2->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading lxml-5.3.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb==0.19.0->-r requirements.txt (line 5)) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=2.6->wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.1 (from pydantic<3,>=2.6->wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading pydantic_core-2.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb==0.19.0->-r requirements.txt (line 5)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb==0.19.0->-r requirements.txt (line 5)) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb==0.19.0->-r requirements.txt (line 5)) (2024.8.30)\n",
      "Collecting pyarrow>=15.0.0 (from datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading pyarrow-18.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting xxhash (from datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading aiohttp-3.11.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from datasets->torchtune==0.4.0->-r requirements.txt (line 4)) (24.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from jinja2->torch==2.5.1->-r requirements.txt (line 1)) (2.1.3)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting regex>=2022.1.18 (from tiktoken->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets->torchtune==0.4.0->-r requirements.txt (line 4)) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading propcache-0.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from pandas->datasets->torchtune==0.4.0->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from pandas->datasets->torchtune==0.4.0->-r requirements.txt (line 4)) (2024.1)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Downloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl (906.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m116.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchao-0.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m139.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.20.1-cp312-cp312-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m215.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchtune-0.4.0-py3-none-any.whl (686 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m686.9/686.9 kB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wandb-0.19.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m202.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m222.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m224.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m241.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m171.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m272.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m250.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m265.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m275.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m252.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m206.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m190.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m236.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading blobfile-3.0.0-py3-none-any.whl (75 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Downloading pillow-11.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m178.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading pydantic-2.10.3-py3-none-any.whl (456 kB)\n",
      "Downloading pydantic_core-2.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m141.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-2.19.2-py2.py3-none-any.whl (322 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Downloading huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\n",
      "Downloading numpy-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m213.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Downloading safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (434 kB)\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
      "Downloading tiktoken-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading aiohttp-3.11.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m120.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Downloading lxml-5.3.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m172.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading pyarrow-18.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (40.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m249.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m124.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m197.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\n",
      "Downloading multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
      "Downloading propcache-0.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (243 kB)\n",
      "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Downloading yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (336 kB)\n",
      "Building wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=b84cc45a8a9b23b7c8e95033fb57fa8e2c59075740c0cbbe95703300e1914ada\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/1f/be/48/13754633f1d08d1fbfc60d5e80ae1e5d7329500477685286cd\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: torchao, sentencepiece, mpmath, antlr4-python3-runtime, xxhash, tzdata, typing-extensions, sympy, smmap, setproctitle, sentry-sdk, safetensors, regex, python-dotenv, pycryptodomex, pyarrow, protobuf, propcache, pillow, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, lxml, fsspec, frozenlist, filelock, docker-pycreds, dill, click, annotated-types, aiohappyeyeballs, yarl, triton, tiktoken, pydantic-core, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, huggingface-hub, gitdb, blobfile, aiosignal, pydantic, nvidia-cusolver-cu12, gitpython, aiohttp, wandb, torch, torchvision, datasets, torchtune\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "Successfully installed aiohappyeyeballs-2.4.4 aiohttp-3.11.10 aiosignal-1.3.1 annotated-types-0.7.0 antlr4-python3-runtime-4.9.3 blobfile-3.0.0 click-8.1.7 datasets-3.2.0 dill-0.3.8 docker-pycreds-0.4.0 filelock-3.16.1 frozenlist-1.5.0 fsspec-2024.9.0 gitdb-4.0.11 gitpython-3.1.43 huggingface-hub-0.26.5 lxml-5.3.0 mpmath-1.3.0 multidict-6.1.0 multiprocess-0.70.16 networkx-3.4.2 numpy-2.2.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 omegaconf-2.3.0 pandas-2.2.3 pillow-11.0.0 propcache-0.2.1 protobuf-5.29.1 pyarrow-18.1.0 pycryptodomex-3.21.0 pydantic-2.10.3 pydantic-core-2.27.1 python-dotenv-1.0.1 regex-2024.11.6 safetensors-0.4.5 sentencepiece-0.2.0 sentry-sdk-2.19.2 setproctitle-1.3.4 smmap-5.0.1 sympy-1.13.1 tiktoken-0.8.0 torch-2.5.1 torchao-0.6.1 torchtune-0.4.0 torchvision-0.20.1 triton-3.1.0 typing-extensions-4.12.2 tzdata-2024.2 wandb-0.19.0 xxhash-3.5.0 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "ipython = get_ipython()\n",
    "@register_cell_magic\n",
    "def pybash(line, cell):\n",
    "    ipython.run_cell_magic('bash', '', cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONFIG\n",
    "NUM_GPUS = 4\n",
    "HF_TOKEN = os.environ[\"HF_TOKEN_R\"]\n",
    "IGNORE_PATTERNS = \"original/consolidated*\"\n",
    "CONFIG_FILE = \"llama_3_1_8b_lora_distributed.yaml\"\n",
    "\n",
    "## MODEL\n",
    "FT_MODEL_REPO = \"multimodalai\"\n",
    "BASE_MODEL_HF_ID = \"meta-llama/Llama-3.1-8B\"\n",
    "CLIENT = \"resume-critique\"\n",
    "MODEL = \"llama3_1_8b\"\n",
    "MODEL_NUMBER = \"5\"\n",
    "REV_N = \"1\"\n",
    "FT_METHOD = \"tt_lora\"\n",
    "MODEL_TYPE = \"adapter\"\n",
    "\n",
    "MDATA_ID = f\"model_{MODEL_NUMBER}_20k\"\n",
    "REV = f\"rev_{REV_N}\"\n",
    "FT_MODEL_NAME = f\"{CLIENT}-{MODEL}-{FT_METHOD}-{MDATA_ID}-{MODEL_TYPE}-{REV}\"\n",
    "FT_MODEL_HF_ID = f\"multimodalai/{FT_MODEL_NAME}\"\n",
    "\n",
    "## DATASET\n",
    "TRAINING_DATA = \"resume_critique_model_5.jsonl\"\n",
    "\n",
    "## PATH\n",
    "BASE_MODEL_PATH = \"base_model/\"\n",
    "TOKENIZER_PATH = f\"{BASE_MODEL_PATH}/original/tokenizer.model\"\n",
    "OUTPUT_MODEL_PATH = f\"checkpoint/{FT_MODEL_REPO}/{FT_MODEL_NAME}\"\n",
    "TRAINING_DATA_PATH = f\"data/{TRAINING_DATA}\"\n",
    "CONFIG_FILE_PATH = f\"config/{CONFIG_FILE}\"\n",
    "\n",
    "## TRACKING\n",
    "WANDB_GROUP_NAME = CLIENT\n",
    "RUN_WANDB_NAME = f\"run-{FT_MODEL_NAME}\"\n",
    "LOGS_PATH = \"logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {OUTPUT_MODEL_PATH}\n",
    "!mkdir -p {LOGS_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring files matching the following patterns: original/consolidated*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 16 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [01:58<00:00,  7.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded model repo and wrote to the following locations:\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_5_rev_1/base_model/special_tokens_map.json\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_5_rev_1/base_model/.gitattributes\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_5_rev_1/base_model/tokenizer.json\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_5_rev_1/base_model/LICENSE\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_5_rev_1/base_model/model.safetensors.index.json\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_5_rev_1/base_model/tokenizer_config.json\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_5_rev_1/base_model/model-00004-of-00004.safetensors\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_5_rev_1/base_model/README.md\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_5_rev_1/base_model/original\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_5_rev_1/base_model/USE_POLICY.md\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_5_rev_1/base_model/generation_config.json\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_5_rev_1/base_model/config.json\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_5_rev_1/base_model/model-00003-of-00004.safetensors\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_5_rev_1/base_model/.cache\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_5_rev_1/base_model/model-00001-of-00004.safetensors\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_5_rev_1/base_model/model-00002-of-00004.safetensors\n"
     ]
    }
   ],
   "source": [
    "%%pybash\n",
    "tune download {BASE_MODEL_HF_ID} --output-dir {BASE_MODEL_PATH} --ignore-patterns {IGNORE_PATTERNS} --hf-token {HF_TOKEN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(TRAINING_DATA_PATH, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "You are a professional resume evaluator. Given the following resume evaluation 'Input', please return a conclusion / summarization 'Response' about the given resume critique.\n",
      "\n",
      "### Input:\n",
      "Introduction: The content of your resume is just as important as its format, and there is room for improvement in how your qualifications are presented. While you have a wealth of experience, itâ€™s crucial to ensure that your content engages the reader and effectively highlights your unique value propositions. A focused approach will make your credentials more compelling.\n",
      "Summary: Your professional summary is a strong component of your resume, showcasing your extensive experience and technical skills. However, it could benefit from refinement to ensure itâ€™s both concise and impactful. A well-crafted summary grabs the attention of hiring managers and gives them a snapshot of what you bring to the table. A skill summary is vital for capturing the hiring manager's attention and providing a quick overview of your qualifications. This section serves as a hook that encourages potential employers to read further into your resume. A focused, concise summary can highlight your most relevant skills and achievements, making a lasting impression.\n",
      "Key Words: A dedicated section for key skills and competencies is essential in todayâ€™s job market. If such a section is present, itâ€™s important to ensure it emphasizes hard skills that align with your career goals. If itâ€™s absent, consider adding a robust list of relevant hard skills to enhance your resume's effectiveness.\n",
      "Work History: Your work history is a critical part of your resume, and it should go beyond just listing job titles and company names. Presenting your professional experience in an engaging manner is vital for catching the attention of hiring managers. Detailed descriptions of your roles and accomplishments can significantly strengthen this section.Your work history is consistent and highlights your extensive experience, which is a positive aspect. However, if there are any older positions or gaps, those could potentially raise flags for recruiters. Maintaining a modern focus on your most recent and relevant roles will help mitigate any concerns related to ageism.\n",
      "\n",
      "### Response:\n",
      "To improve your resume's effectiveness, focus on enhancing the content and presentation of your work history. Highlighting achievements and using dynamic language will make your resume more engaging and appealing to potential employers.\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[100].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with torchrun...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1213 02:29:42.877000 12028 site-packages/torch/distributed/run.py:793] \n",
      "W1213 02:29:42.877000 12028 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "W1213 02:29:42.877000 12028 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1213 02:29:42.877000 12028 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "INFO:torchtune.utils._logging:Running LoRAFinetuneRecipeDistributed with resolved config:\n",
      "\n",
      "batch_size: 8\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: base_model/\n",
      "  checkpoint_files:\n",
      "  - model-00001-of-00004.safetensors\n",
      "  - model-00002-of-00004.safetensors\n",
      "  - model-00003-of-00004.safetensors\n",
      "  - model-00004-of-00004.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_5_20k-adapter-rev_1\n",
      "  recipe_checkpoint: null\n",
      "compile: false\n",
      "dataset:\n",
      "  _component_: torchtune.datasets.alpaca_dataset\n",
      "  data_files: data/resume_critique_model_5.jsonl\n",
      "  packed: false\n",
      "  source: json\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: false\n",
      "enable_activation_offloading: false\n",
      "epochs: 3\n",
      "gradient_accumulation_steps: 4\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
      "lr_scheduler:\n",
      "  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup\n",
      "  num_warmup_steps: 30\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.WandBLogger\n",
      "  group: resume-critique\n",
      "  log_dir: logs/\n",
      "  name: run-resume-critique-llama3_1_8b-tt_lora-model_5_20k-adapter-rev_1\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1.lora_llama3_1_8b\n",
      "  apply_lora_to_mlp: true\n",
      "  apply_lora_to_output: true\n",
      "  lora_alpha: 16\n",
      "  lora_attn_modules:\n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  lora_dropout: 0.05\n",
      "  lora_rank: 8\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 0.0003\n",
      "  weight_decay: 0.01\n",
      "output_dir: checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_5_20k-adapter-rev_1\n",
      "profiler:\n",
      "  _component_: torchtune.training.setup_torch_profiler\n",
      "  active_steps: 2\n",
      "  cpu: true\n",
      "  cuda: true\n",
      "  enabled: false\n",
      "  num_cycles: 1\n",
      "  output_dir: checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_5_20k-adapter-rev_1/profiling_outputs\n",
      "  profile_memory: false\n",
      "  record_shapes: true\n",
      "  wait_steps: 5\n",
      "  warmup_steps: 3\n",
      "  with_flops: false\n",
      "  with_stack: false\n",
      "resume_from_checkpoint: false\n",
      "save_adapter_weights_only: false\n",
      "seed: 42\n",
      "shuffle: true\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.llama3.llama3_tokenizer\n",
      "  max_seq_len: 512\n",
      "  path: base_model//original/tokenizer.model\n",
      "\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: leonardo-mm (multimodalcompany). Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.0\n",
      "wandb: Run data is saved locally in logs/wandb/run-20241213_022947-ihnhpkt7\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run run-resume-critique-llama3_1_8b-tt_lora-model_5_20k-adapter-rev_1\n",
      "wandb: â­ï¸ View project at https://wandb.ai/multimodalcompany/torchtune\n",
      "wandb: ğŸš€ View run at https://wandb.ai/multimodalcompany/torchtune/runs/ihnhpkt7\n",
      "INFO:torchtune.utils._logging:Logging base_model/torchtune_config.yaml to W&B under Files\n",
      "INFO:torchtune.utils._logging:FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n",
      "INFO:torchtune.utils._logging:Instantiating model and loading checkpoint took 4.58 secs\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 4.76 GiB\n",
      "\tGPU peak memory reserved: 4.86 GiB\n",
      "\tGPU peak memory active: 4.76 GiB\n",
      "INFO:torchtune.utils._logging:Optimizer is initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
      "INFO:torchtune.utils._logging:Learning rate scheduler is initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "1|151|Loss: 0.6478215456008911: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 151/151 [17:34<00:00,  6.98s/it]INFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...\n",
      "INFO:torchtune.utils._logging:Getting full model state dict took 19.53 secs\n",
      "INFO:torchtune.utils._logging:Retrieving optimizer state dict...\n",
      "INFO:torchtune.utils._logging:Getting optimizer state dict took 19.82 secs\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.98 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_5_20k-adapter-rev_1/hf_model_0001_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 5.00 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_5_20k-adapter-rev_1/hf_model_0002_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.92 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_5_20k-adapter-rev_1/hf_model_0003_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 1.17 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_5_20k-adapter-rev_1/hf_model_0004_0.pt\n",
      "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.04 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_5_20k-adapter-rev_1/adapter_0.pt\n",
      "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.04 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_5_20k-adapter-rev_1/adapter_model.bin\n",
      "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.00 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_5_20k-adapter-rev_1/adapter_config.json\n",
      "INFO:torchtune.utils._logging:Recipe checkpoint of size 0.08 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_5_20k-adapter-rev_1/recipe_state.pt\n",
      "INFO:torchtune.utils._logging:Saving checkpoint took 25.48 secs\n",
      "\n",
      "1|151|Loss: 0.6478215456008911: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 151/151 [18:21<00:00,  7.29s/it]\n",
      "\n",
      "  1%|          | 1/151 [00:06<17:27,  6.99s/it]\u001b[A\n",
      "2|152|Loss: 0.6507548689842224:   1%|          | 1/151 [00:06<17:27,  6.99s/it]\u001b[A\n",
      "2|152|Loss: 0.6507548689842224:   1%|â–         | 2/151 [00:13<17:21,  6.99s/it]\u001b[A\n",
      "2|153|Loss: 0.64940345287323:   1%|â–         | 2/151 [00:13<17:21,  6.99s/it]  \u001b[A\n",
      "2|153|Loss: 0.64940345287323:   2%|â–         | 3/151 [00:20<17:15,  6.99s/it]\u001b[A\n",
      "2|154|Loss: 0.6377192735671997:   2%|â–         | 3/151 [00:20<17:15,  6.99s/it]\u001b[A\n",
      "2|154|Loss: 0.6377192735671997:   3%|â–         | 4/151 [00:27<17:08,  7.00s/it]\u001b[A\n",
      "2|155|Loss: 0.6341484785079956:   3%|â–         | 4/151 [00:27<17:08,  7.00s/it]\u001b[A\n",
      "2|155|Loss: 0.6341484785079956:   3%|â–         | 5/151 [00:35<17:04,  7.02s/it]\u001b[A\n",
      "2|156|Loss: 0.6510697603225708:   3%|â–         | 5/151 [00:35<17:04,  7.02s/it]\u001b[A\n",
      "2|156|Loss: 0.6510697603225708:   4%|â–         | 6/151 [00:42<16:55,  7.00s/it]\u001b[A\n",
      "2|157|Loss: 0.639146625995636:   4%|â–         | 6/151 [00:42<16:55,  7.00s/it] \u001b[A\n",
      "2|157|Loss: 0.639146625995636:   5%|â–         | 7/151 [00:48<16:47,  7.00s/it]\u001b[A\n",
      "2|158|Loss: 0.6458755135536194:   5%|â–         | 7/151 [00:48<16:47,  7.00s/it]\u001b[A\n",
      "2|158|Loss: 0.6458755135536194:   5%|â–Œ         | 8/151 [00:55<16:39,  6.99s/it]\u001b[A\n",
      "2|159|Loss: 0.6468778252601624:   5%|â–Œ         | 8/151 [00:55<16:39,  6.99s/it]\u001b[A\n",
      "2|159|Loss: 0.6468778252601624:   6%|â–Œ         | 9/151 [01:02<16:31,  6.98s/it]\u001b[A\n",
      "2|160|Loss: 0.6510633230209351:   6%|â–Œ         | 9/151 [01:02<16:31,  6.98s/it]\u001b[A\n",
      "2|160|Loss: 0.6510633230209351:   7%|â–‹         | 10/151 [01:09<16:26,  7.00s/it]\u001b[A\n",
      "2|161|Loss: 0.6369231939315796:   7%|â–‹         | 10/151 [01:09<16:26,  7.00s/it]\u001b[A\n",
      "2|161|Loss: 0.6369231939315796:   7%|â–‹         | 11/151 [01:16<16:18,  6.99s/it]\u001b[A\n",
      "2|162|Loss: 0.6321433186531067:   7%|â–‹         | 11/151 [01:16<16:18,  6.99s/it]\u001b[A\n",
      "2|162|Loss: 0.6321433186531067:   8%|â–Š         | 12/151 [01:23<16:13,  7.00s/it]\u001b[A\n",
      "2|163|Loss: 0.6560879945755005:   8%|â–Š         | 12/151 [01:23<16:13,  7.00s/it]\u001b[A\n",
      "2|163|Loss: 0.6560879945755005:   9%|â–Š         | 13/151 [01:30<16:04,  6.99s/it]\u001b[A\n",
      "2|164|Loss: 0.6436089873313904:   9%|â–Š         | 13/151 [01:30<16:04,  6.99s/it]\u001b[A\n",
      "2|164|Loss: 0.6436089873313904:   9%|â–‰         | 14/151 [01:37<15:55,  6.98s/it]\u001b[A\n",
      "2|165|Loss: 0.636934757232666:   9%|â–‰         | 14/151 [01:37<15:55,  6.98s/it] \u001b[A\n",
      "2|165|Loss: 0.636934757232666:  10%|â–‰         | 15/151 [01:44<15:51,  7.00s/it]\u001b[A\n",
      "2|166|Loss: 0.6462123990058899:  10%|â–‰         | 15/151 [01:44<15:51,  7.00s/it]\u001b[A\n",
      "2|166|Loss: 0.6462123990058899:  11%|â–ˆ         | 16/151 [01:51<15:44,  7.00s/it]\u001b[A\n",
      "2|167|Loss: 0.6499061584472656:  11%|â–ˆ         | 16/151 [01:51<15:44,  7.00s/it]\u001b[A\n",
      "2|167|Loss: 0.6499061584472656:  11%|â–ˆâ–        | 17/151 [01:58<15:37,  6.99s/it]\u001b[A\n",
      "2|168|Loss: 0.6507247686386108:  11%|â–ˆâ–        | 17/151 [01:58<15:37,  6.99s/it]\u001b[A\n",
      "2|168|Loss: 0.6507247686386108:  12%|â–ˆâ–        | 18/151 [02:05<15:30,  6.99s/it]\u001b[A\n",
      "2|169|Loss: 0.6512097716331482:  12%|â–ˆâ–        | 18/151 [02:05<15:30,  6.99s/it]\u001b[A\n",
      "2|169|Loss: 0.6512097716331482:  13%|â–ˆâ–        | 19/151 [02:12<15:20,  6.98s/it]\u001b[A\n",
      "2|170|Loss: 0.6390615701675415:  13%|â–ˆâ–        | 19/151 [02:12<15:20,  6.98s/it]\u001b[A\n",
      "2|170|Loss: 0.6390615701675415:  13%|â–ˆâ–        | 20/151 [02:19<15:16,  6.99s/it]\u001b[A\n",
      "2|171|Loss: 0.648291289806366:  13%|â–ˆâ–        | 20/151 [02:19<15:16,  6.99s/it] \u001b[A\n",
      "2|171|Loss: 0.648291289806366:  14%|â–ˆâ–        | 21/151 [02:26<15:09,  6.99s/it]\u001b[A\n",
      "2|172|Loss: 0.650465190410614:  14%|â–ˆâ–        | 21/151 [02:26<15:09,  6.99s/it]\u001b[A\n",
      "2|172|Loss: 0.650465190410614:  15%|â–ˆâ–        | 22/151 [02:33<15:01,  6.99s/it]\u001b[A\n",
      "2|173|Loss: 0.6369417309761047:  15%|â–ˆâ–        | 22/151 [02:33<15:01,  6.99s/it]\u001b[A\n",
      "2|173|Loss: 0.6369417309761047:  15%|â–ˆâ–Œ        | 23/151 [02:40<14:54,  6.99s/it]\u001b[A\n",
      "2|174|Loss: 0.6480250954627991:  15%|â–ˆâ–Œ        | 23/151 [02:40<14:54,  6.99s/it]\u001b[A\n",
      "2|174|Loss: 0.6480250954627991:  16%|â–ˆâ–Œ        | 24/151 [02:47<14:47,  6.99s/it]\u001b[A\n",
      "2|175|Loss: 0.639322817325592:  16%|â–ˆâ–Œ        | 24/151 [02:47<14:47,  6.99s/it] \u001b[A\n",
      "2|175|Loss: 0.639322817325592:  17%|â–ˆâ–‹        | 25/151 [02:54<14:41,  6.99s/it]\u001b[A\n",
      "2|176|Loss: 0.6414147019386292:  17%|â–ˆâ–‹        | 25/151 [02:54<14:41,  6.99s/it]\u001b[A\n",
      "2|176|Loss: 0.6414147019386292:  17%|â–ˆâ–‹        | 26/151 [03:01<14:33,  6.99s/it]\u001b[A\n",
      "2|177|Loss: 0.6388698220252991:  17%|â–ˆâ–‹        | 26/151 [03:01<14:33,  6.99s/it]\u001b[A\n",
      "2|177|Loss: 0.6388698220252991:  18%|â–ˆâ–Š        | 27/151 [03:08<14:26,  6.99s/it]\u001b[A\n",
      "2|178|Loss: 0.6483887434005737:  18%|â–ˆâ–Š        | 27/151 [03:08<14:26,  6.99s/it]\u001b[A\n",
      "2|178|Loss: 0.6483887434005737:  19%|â–ˆâ–Š        | 28/151 [03:15<14:21,  7.00s/it]\u001b[A\n",
      "2|179|Loss: 0.6486252546310425:  19%|â–ˆâ–Š        | 28/151 [03:15<14:21,  7.00s/it]\u001b[A\n",
      "2|179|Loss: 0.6486252546310425:  19%|â–ˆâ–‰        | 29/151 [03:22<14:14,  7.00s/it]\u001b[A\n",
      "2|180|Loss: 0.6399720907211304:  19%|â–ˆâ–‰        | 29/151 [03:22<14:14,  7.00s/it]\u001b[A\n",
      "2|180|Loss: 0.6399720907211304:  20%|â–ˆâ–‰        | 30/151 [03:29<14:07,  7.00s/it]\u001b[A\n",
      "2|181|Loss: 0.6404820680618286:  20%|â–ˆâ–‰        | 30/151 [03:29<14:07,  7.00s/it]\u001b[A\n",
      "2|181|Loss: 0.6404820680618286:  21%|â–ˆâ–ˆ        | 31/151 [03:36<14:00,  7.01s/it]\u001b[A\n",
      "2|182|Loss: 0.6374371647834778:  21%|â–ˆâ–ˆ        | 31/151 [03:36<14:00,  7.01s/it]\u001b[A\n",
      "2|182|Loss: 0.6374371647834778:  21%|â–ˆâ–ˆ        | 32/151 [03:43<13:53,  7.01s/it]\u001b[A\n",
      "2|183|Loss: 0.6325773596763611:  21%|â–ˆâ–ˆ        | 32/151 [03:43<13:53,  7.01s/it]\u001b[A\n",
      "2|183|Loss: 0.6325773596763611:  22%|â–ˆâ–ˆâ–       | 33/151 [03:50<13:45,  7.00s/it]\u001b[A\n",
      "2|184|Loss: 0.6423727869987488:  22%|â–ˆâ–ˆâ–       | 33/151 [03:50<13:45,  7.00s/it]\u001b[A\n",
      "2|184|Loss: 0.6423727869987488:  23%|â–ˆâ–ˆâ–       | 34/151 [03:57<13:37,  6.98s/it]\u001b[A\n",
      "2|185|Loss: 0.6433779001235962:  23%|â–ˆâ–ˆâ–       | 34/151 [03:57<13:37,  6.98s/it]\u001b[A\n",
      "2|185|Loss: 0.6433779001235962:  23%|â–ˆâ–ˆâ–       | 35/151 [04:04<13:29,  6.98s/it]\u001b[A\n",
      "2|186|Loss: 0.6398953795433044:  23%|â–ˆâ–ˆâ–       | 35/151 [04:04<13:29,  6.98s/it]\u001b[A\n",
      "2|186|Loss: 0.6398953795433044:  24%|â–ˆâ–ˆâ–       | 36/151 [04:11<13:23,  6.99s/it]\u001b[A\n",
      "2|187|Loss: 0.6317988038063049:  24%|â–ˆâ–ˆâ–       | 36/151 [04:11<13:23,  6.99s/it]\u001b[A\n",
      "2|187|Loss: 0.6317988038063049:  25%|â–ˆâ–ˆâ–       | 37/151 [04:18<13:15,  6.98s/it]\u001b[A\n",
      "2|188|Loss: 0.6321044564247131:  25%|â–ˆâ–ˆâ–       | 37/151 [04:18<13:15,  6.98s/it]\u001b[A\n",
      "2|188|Loss: 0.6321044564247131:  25%|â–ˆâ–ˆâ–Œ       | 38/151 [04:25<13:09,  6.99s/it]\u001b[A\n",
      "2|189|Loss: 0.6446816921234131:  25%|â–ˆâ–ˆâ–Œ       | 38/151 [04:25<13:09,  6.99s/it]\u001b[A\n",
      "2|189|Loss: 0.6446816921234131:  26%|â–ˆâ–ˆâ–Œ       | 39/151 [04:32<13:02,  6.99s/it]\u001b[A\n",
      "2|190|Loss: 0.6377567648887634:  26%|â–ˆâ–ˆâ–Œ       | 39/151 [04:32<13:02,  6.99s/it]\u001b[A\n",
      "2|190|Loss: 0.6377567648887634:  26%|â–ˆâ–ˆâ–‹       | 40/151 [04:39<12:55,  6.99s/it]\u001b[A\n",
      "2|191|Loss: 0.6486096382141113:  26%|â–ˆâ–ˆâ–‹       | 40/151 [04:39<12:55,  6.99s/it]\u001b[A\n",
      "2|191|Loss: 0.6486096382141113:  27%|â–ˆâ–ˆâ–‹       | 41/151 [04:46<12:49,  6.99s/it]\u001b[A\n",
      "2|192|Loss: 0.6458292007446289:  27%|â–ˆâ–ˆâ–‹       | 41/151 [04:46<12:49,  6.99s/it]\u001b[A\n",
      "2|192|Loss: 0.6458292007446289:  28%|â–ˆâ–ˆâ–Š       | 42/151 [04:53<12:41,  6.99s/it]\u001b[A\n",
      "2|193|Loss: 0.6494301557540894:  28%|â–ˆâ–ˆâ–Š       | 42/151 [04:53<12:41,  6.99s/it]\u001b[A\n",
      "2|193|Loss: 0.6494301557540894:  28%|â–ˆâ–ˆâ–Š       | 43/151 [05:00<12:34,  6.98s/it]\u001b[A\n",
      "2|194|Loss: 0.6384748220443726:  28%|â–ˆâ–ˆâ–Š       | 43/151 [05:00<12:34,  6.98s/it]\u001b[A\n",
      "2|194|Loss: 0.6384748220443726:  29%|â–ˆâ–ˆâ–‰       | 44/151 [05:07<12:27,  6.98s/it]\u001b[A\n",
      "2|195|Loss: 0.6310582160949707:  29%|â–ˆâ–ˆâ–‰       | 44/151 [05:07<12:27,  6.98s/it]\u001b[A\n",
      "2|195|Loss: 0.6310582160949707:  30%|â–ˆâ–ˆâ–‰       | 45/151 [05:14<12:21,  6.99s/it]\u001b[A\n",
      "2|196|Loss: 0.6336585283279419:  30%|â–ˆâ–ˆâ–‰       | 45/151 [05:14<12:21,  6.99s/it]\u001b[A\n",
      "2|196|Loss: 0.6336585283279419:  30%|â–ˆâ–ˆâ–ˆ       | 46/151 [05:21<12:14,  7.00s/it]\u001b[A\n",
      "2|197|Loss: 0.6336899399757385:  30%|â–ˆâ–ˆâ–ˆ       | 46/151 [05:21<12:14,  7.00s/it]\u001b[A\n",
      "2|197|Loss: 0.6336899399757385:  31%|â–ˆâ–ˆâ–ˆ       | 47/151 [05:28<12:06,  6.98s/it]\u001b[A\n",
      "2|198|Loss: 0.6394696831703186:  31%|â–ˆâ–ˆâ–ˆ       | 47/151 [05:28<12:06,  6.98s/it]\u001b[A\n",
      "2|198|Loss: 0.6394696831703186:  32%|â–ˆâ–ˆâ–ˆâ–      | 48/151 [05:35<12:00,  6.99s/it]\u001b[A\n",
      "2|199|Loss: 0.6555122137069702:  32%|â–ˆâ–ˆâ–ˆâ–      | 48/151 [05:35<12:00,  6.99s/it]\u001b[A\n",
      "2|199|Loss: 0.6555122137069702:  32%|â–ˆâ–ˆâ–ˆâ–      | 49/151 [05:42<11:54,  7.00s/it]\u001b[A\n",
      "2|200|Loss: 0.6415006518363953:  32%|â–ˆâ–ˆâ–ˆâ–      | 49/151 [05:42<11:54,  7.00s/it]\u001b[A\n",
      "2|200|Loss: 0.6415006518363953:  33%|â–ˆâ–ˆâ–ˆâ–      | 50/151 [05:49<11:46,  7.00s/it]\u001b[A\n",
      "2|201|Loss: 0.6474838852882385:  33%|â–ˆâ–ˆâ–ˆâ–      | 50/151 [05:49<11:46,  7.00s/it]\u001b[A\n",
      "2|201|Loss: 0.6474838852882385:  34%|â–ˆâ–ˆâ–ˆâ–      | 51/151 [05:56<11:39,  6.99s/it]\u001b[A\n",
      "2|202|Loss: 0.6399528980255127:  34%|â–ˆâ–ˆâ–ˆâ–      | 51/151 [05:56<11:39,  6.99s/it]\u001b[A\n",
      "2|202|Loss: 0.6399528980255127:  34%|â–ˆâ–ˆâ–ˆâ–      | 52/151 [06:03<11:32,  7.00s/it]\u001b[A\n",
      "2|203|Loss: 0.629879891872406:  34%|â–ˆâ–ˆâ–ˆâ–      | 52/151 [06:03<11:32,  7.00s/it] \u001b[A\n",
      "2|203|Loss: 0.629879891872406:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 53/151 [06:10<11:25,  7.00s/it]\u001b[A\n",
      "2|204|Loss: 0.6464441418647766:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 53/151 [06:10<11:25,  7.00s/it]\u001b[A\n",
      "2|204|Loss: 0.6464441418647766:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 54/151 [06:17<11:20,  7.01s/it]\u001b[A\n",
      "2|205|Loss: 0.6367211937904358:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 54/151 [06:17<11:20,  7.01s/it]\u001b[A\n",
      "2|205|Loss: 0.6367211937904358:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 55/151 [06:24<11:12,  7.00s/it]\u001b[A\n",
      "2|206|Loss: 0.6299872398376465:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 55/151 [06:24<11:12,  7.00s/it]\u001b[A\n",
      "2|206|Loss: 0.6299872398376465:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 56/151 [06:31<11:04,  7.00s/it]\u001b[A\n",
      "2|207|Loss: 0.6366922855377197:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 56/151 [06:31<11:04,  7.00s/it]\u001b[A\n",
      "2|207|Loss: 0.6366922855377197:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 57/151 [06:38<10:57,  7.00s/it]\u001b[A\n",
      "2|208|Loss: 0.6463238000869751:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 57/151 [06:38<10:57,  7.00s/it]\u001b[A\n",
      "2|208|Loss: 0.6463238000869751:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 58/151 [06:45<10:50,  6.99s/it]\u001b[A\n",
      "2|209|Loss: 0.6377905011177063:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 58/151 [06:45<10:50,  6.99s/it]\u001b[A\n",
      "2|209|Loss: 0.6377905011177063:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 59/151 [06:52<10:43,  7.00s/it]\u001b[A\n",
      "2|210|Loss: 0.6439698934555054:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 59/151 [06:52<10:43,  7.00s/it]\u001b[A\n",
      "2|210|Loss: 0.6439698934555054:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 60/151 [06:59<10:36,  6.99s/it]\u001b[A\n",
      "2|211|Loss: 0.6382736563682556:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 60/151 [06:59<10:36,  6.99s/it]\u001b[A\n",
      "2|211|Loss: 0.6382736563682556:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 61/151 [07:06<10:28,  6.99s/it]\u001b[A\n",
      "2|212|Loss: 0.6357418894767761:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 61/151 [07:06<10:28,  6.99s/it]\u001b[A\n",
      "2|212|Loss: 0.6357418894767761:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 62/151 [07:13<10:22,  7.00s/it]\u001b[A\n",
      "2|213|Loss: 0.6333624124526978:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 62/151 [07:13<10:22,  7.00s/it]\u001b[A\n",
      "2|213|Loss: 0.6333624124526978:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 63/151 [07:20<10:16,  7.00s/it]\u001b[A\n",
      "2|214|Loss: 0.6329151391983032:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 63/151 [07:20<10:16,  7.00s/it]\u001b[A\n",
      "2|214|Loss: 0.6329151391983032:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 64/151 [07:27<10:09,  7.01s/it]\u001b[A\n",
      "2|215|Loss: 0.6264016032218933:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 64/151 [07:27<10:09,  7.01s/it]\u001b[A\n",
      "2|215|Loss: 0.6264016032218933:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 65/151 [07:34<10:02,  7.00s/it]\u001b[A\n",
      "2|216|Loss: 0.6355836987495422:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 65/151 [07:34<10:02,  7.00s/it]\u001b[A\n",
      "2|216|Loss: 0.6355836987495422:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 66/151 [07:41<09:55,  7.00s/it]\u001b[A\n",
      "2|217|Loss: 0.640805184841156:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 66/151 [07:41<09:55,  7.00s/it] \u001b[A\n",
      "2|217|Loss: 0.640805184841156:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 67/151 [07:48<09:48,  7.01s/it]\u001b[A\n",
      "2|218|Loss: 0.6368855834007263:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 67/151 [07:48<09:48,  7.01s/it]\u001b[A\n",
      "2|218|Loss: 0.6368855834007263:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 68/151 [07:55<09:42,  7.01s/it]\u001b[A\n",
      "2|219|Loss: 0.6415361166000366:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 68/151 [07:55<09:42,  7.01s/it]\u001b[A\n",
      "2|219|Loss: 0.6415361166000366:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 69/151 [08:02<09:33,  7.00s/it]\u001b[A\n",
      "2|220|Loss: 0.6350038051605225:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 69/151 [08:02<09:33,  7.00s/it]\u001b[A\n",
      "2|220|Loss: 0.6350038051605225:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 70/151 [08:09<09:26,  7.00s/it]\u001b[A\n",
      "2|221|Loss: 0.6390760540962219:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 70/151 [08:09<09:26,  7.00s/it]\u001b[A\n",
      "2|221|Loss: 0.6390760540962219:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 71/151 [08:16<09:19,  6.99s/it]\u001b[A\n",
      "2|222|Loss: 0.6296163201332092:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 71/151 [08:16<09:19,  6.99s/it]\u001b[A\n",
      "2|222|Loss: 0.6296163201332092:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 72/151 [08:23<09:13,  7.01s/it]\u001b[A\n",
      "2|223|Loss: 0.6440919041633606:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 72/151 [08:23<09:13,  7.01s/it]\u001b[A\n",
      "2|223|Loss: 0.6440919041633606:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 73/151 [08:30<09:05,  7.00s/it]\u001b[A\n",
      "2|224|Loss: 0.6278928518295288:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 73/151 [08:30<09:05,  7.00s/it]\u001b[A\n",
      "2|224|Loss: 0.6278928518295288:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 74/151 [08:37<08:58,  6.99s/it]\u001b[A\n",
      "2|225|Loss: 0.6319393515586853:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 74/151 [08:37<08:58,  6.99s/it]\u001b[A\n",
      "2|225|Loss: 0.6319393515586853:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 75/151 [08:44<08:51,  7.00s/it]\u001b[A\n",
      "2|226|Loss: 0.6337968707084656:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 75/151 [08:44<08:51,  7.00s/it]\u001b[A\n",
      "2|226|Loss: 0.6337968707084656:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 76/151 [08:51<08:44,  6.99s/it]\u001b[A\n",
      "2|227|Loss: 0.6394491791725159:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 76/151 [08:51<08:44,  6.99s/it]\u001b[A\n",
      "2|227|Loss: 0.6394491791725159:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 77/151 [08:58<08:37,  6.99s/it]\u001b[A\n",
      "2|228|Loss: 0.6364163756370544:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 77/151 [08:58<08:37,  6.99s/it]\u001b[A\n",
      "2|228|Loss: 0.6364163756370544:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78/151 [09:05<08:30,  6.99s/it]\u001b[A\n",
      "2|229|Loss: 0.640757143497467:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78/151 [09:05<08:30,  6.99s/it] \u001b[A\n",
      "2|229|Loss: 0.640757143497467:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 79/151 [09:12<08:23,  6.99s/it]\u001b[A\n",
      "2|230|Loss: 0.6282505989074707:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 79/151 [09:12<08:23,  6.99s/it]\u001b[A\n",
      "2|230|Loss: 0.6282505989074707:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 80/151 [09:19<08:16,  6.99s/it]\u001b[A\n",
      "2|231|Loss: 0.6405777335166931:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 80/151 [09:19<08:16,  6.99s/it]\u001b[A\n",
      "2|231|Loss: 0.6405777335166931:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 81/151 [09:26<08:09,  6.99s/it]\u001b[A\n",
      "2|232|Loss: 0.6499848365783691:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 81/151 [09:26<08:09,  6.99s/it]\u001b[A\n",
      "2|232|Loss: 0.6499848365783691:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 82/151 [09:33<08:02,  7.00s/it]\u001b[A\n",
      "2|233|Loss: 0.6383031010627747:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 82/151 [09:33<08:02,  7.00s/it]\u001b[A\n",
      "2|233|Loss: 0.6383031010627747:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 83/151 [09:40<07:56,  7.00s/it]\u001b[A\n",
      "2|234|Loss: 0.6358960866928101:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 83/151 [09:40<07:56,  7.00s/it]\u001b[A\n",
      "2|234|Loss: 0.6358960866928101:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 84/151 [09:47<07:49,  7.01s/it]\u001b[A\n",
      "2|235|Loss: 0.6322672367095947:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 84/151 [09:47<07:49,  7.01s/it]\u001b[A\n",
      "2|235|Loss: 0.6322672367095947:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 85/151 [09:54<07:42,  7.01s/it]\u001b[A\n",
      "2|236|Loss: 0.6417930722236633:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 85/151 [09:54<07:42,  7.01s/it]\u001b[A\n",
      "2|236|Loss: 0.6417930722236633:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 86/151 [10:01<07:34,  7.00s/it]\u001b[A\n",
      "2|237|Loss: 0.6277002692222595:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 86/151 [10:01<07:34,  7.00s/it]\u001b[A\n",
      "2|237|Loss: 0.6277002692222595:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 87/151 [10:08<07:27,  7.00s/it]\u001b[A\n",
      "2|238|Loss: 0.64094477891922:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 87/151 [10:08<07:27,  7.00s/it]  \u001b[A\n",
      "2|238|Loss: 0.64094477891922:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 88/151 [10:15<07:21,  7.00s/it]\u001b[A\n",
      "2|239|Loss: 0.6437628865242004:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 88/151 [10:15<07:21,  7.00s/it]\u001b[A\n",
      "2|239|Loss: 0.6437628865242004:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 89/151 [10:22<07:13,  6.99s/it]\u001b[A\n",
      "2|240|Loss: 0.6368590593338013:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 89/151 [10:22<07:13,  6.99s/it]\u001b[A\n",
      "2|240|Loss: 0.6368590593338013:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 90/151 [10:29<07:07,  7.01s/it]\u001b[A\n",
      "2|241|Loss: 0.6208961606025696:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 90/151 [10:29<07:07,  7.01s/it]\u001b[A\n",
      "2|241|Loss: 0.6208961606025696:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 91/151 [10:36<07:00,  7.00s/it]\u001b[A\n",
      "2|242|Loss: 0.6310509443283081:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 91/151 [10:36<07:00,  7.00s/it]\u001b[A\n",
      "2|242|Loss: 0.6310509443283081:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 92/151 [10:43<06:52,  7.00s/it]\u001b[A\n",
      "2|243|Loss: 0.6211695671081543:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 92/151 [10:43<06:52,  7.00s/it]\u001b[A\n",
      "2|243|Loss: 0.6211695671081543:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 93/151 [10:50<06:45,  7.00s/it]\u001b[A\n",
      "2|244|Loss: 0.6367849111557007:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 93/151 [10:50<06:45,  7.00s/it]\u001b[A\n",
      "2|244|Loss: 0.6367849111557007:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 94/151 [10:57<06:38,  6.98s/it]\u001b[A\n",
      "2|245|Loss: 0.6319451928138733:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 94/151 [10:57<06:38,  6.98s/it]\u001b[A\n",
      "2|245|Loss: 0.6319451928138733:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 95/151 [11:04<06:30,  6.98s/it]\u001b[A\n",
      "2|246|Loss: 0.6255351305007935:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 95/151 [11:04<06:30,  6.98s/it]\u001b[A\n",
      "2|246|Loss: 0.6255351305007935:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 96/151 [11:11<06:24,  6.98s/it]\u001b[A\n",
      "2|247|Loss: 0.629046618938446:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 96/151 [11:11<06:24,  6.98s/it] \u001b[A\n",
      "2|247|Loss: 0.629046618938446:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 97/151 [11:18<06:16,  6.98s/it]\u001b[A\n",
      "2|248|Loss: 0.6370493769645691:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 97/151 [11:18<06:16,  6.98s/it]\u001b[A\n",
      "2|248|Loss: 0.6370493769645691:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 98/151 [11:25<06:10,  6.99s/it]\u001b[A\n",
      "2|249|Loss: 0.6365377306938171:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 98/151 [11:25<06:10,  6.99s/it]\u001b[A\n",
      "2|249|Loss: 0.6365377306938171:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 99/151 [11:32<06:03,  6.99s/it]\u001b[A\n",
      "2|250|Loss: 0.6296878457069397:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 99/151 [11:32<06:03,  6.99s/it]\u001b[A\n",
      "2|250|Loss: 0.6296878457069397:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 100/151 [11:39<05:56,  6.99s/it]\u001b[A\n",
      "2|251|Loss: 0.6350702047348022:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 100/151 [11:39<05:56,  6.99s/it]\u001b[A\n",
      "2|251|Loss: 0.6350702047348022:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 101/151 [11:46<05:49,  6.99s/it]\u001b[A\n",
      "2|252|Loss: 0.6301213502883911:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 101/151 [11:46<05:49,  6.99s/it]\u001b[A\n",
      "2|252|Loss: 0.6301213502883911:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 102/151 [11:53<05:42,  6.98s/it]\u001b[A\n",
      "2|253|Loss: 0.6320593953132629:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 102/151 [11:53<05:42,  6.98s/it]\u001b[A\n",
      "2|253|Loss: 0.6320593953132629:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 103/151 [12:00<05:35,  6.98s/it]\u001b[A\n",
      "2|254|Loss: 0.6365758776664734:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 103/151 [12:00<05:35,  6.98s/it]\u001b[A\n",
      "2|254|Loss: 0.6365758776664734:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 104/151 [12:07<05:28,  6.99s/it]\u001b[A\n",
      "2|255|Loss: 0.6280969381332397:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 104/151 [12:07<05:28,  6.99s/it]\u001b[A\n",
      "2|255|Loss: 0.6280969381332397:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 105/151 [12:14<05:21,  6.99s/it]\u001b[A\n",
      "2|256|Loss: 0.6322628855705261:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 105/151 [12:14<05:21,  6.99s/it]\u001b[A\n",
      "2|256|Loss: 0.6322628855705261:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 106/151 [12:21<05:14,  7.00s/it]\u001b[A\n",
      "2|257|Loss: 0.6332835555076599:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 106/151 [12:21<05:14,  7.00s/it]\u001b[A\n",
      "2|257|Loss: 0.6332835555076599:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 107/151 [12:28<05:07,  6.99s/it]\u001b[A\n",
      "2|258|Loss: 0.6390435695648193:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 107/151 [12:28<05:07,  6.99s/it]\u001b[A\n",
      "2|258|Loss: 0.6390435695648193:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 108/151 [12:35<05:00,  6.99s/it]\u001b[A\n",
      "2|259|Loss: 0.6392809748649597:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 108/151 [12:35<05:00,  6.99s/it]\u001b[A\n",
      "2|259|Loss: 0.6392809748649597:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 109/151 [12:42<04:53,  6.98s/it]\u001b[A\n",
      "2|260|Loss: 0.6405466198921204:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 109/151 [12:42<04:53,  6.98s/it]\u001b[A\n",
      "2|260|Loss: 0.6405466198921204:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 110/151 [12:49<04:45,  6.97s/it]\u001b[A\n",
      "2|261|Loss: 0.6264007687568665:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 110/151 [12:49<04:45,  6.97s/it]\u001b[A\n",
      "2|261|Loss: 0.6264007687568665:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 111/151 [12:56<04:39,  6.98s/it]\u001b[A\n",
      "2|262|Loss: 0.6290763020515442:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 111/151 [12:56<04:39,  6.98s/it]\u001b[A\n",
      "2|262|Loss: 0.6290763020515442:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 112/151 [13:03<04:32,  6.98s/it]\u001b[A\n",
      "2|263|Loss: 0.6319363713264465:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 112/151 [13:03<04:32,  6.98s/it]\u001b[A\n",
      "2|263|Loss: 0.6319363713264465:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 113/151 [13:10<04:25,  6.98s/it]\u001b[A\n",
      "2|264|Loss: 0.6239116787910461:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 113/151 [13:10<04:25,  6.98s/it]\u001b[A\n",
      "2|264|Loss: 0.6239116787910461:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 114/151 [13:17<04:18,  6.98s/it]\u001b[A\n",
      "2|265|Loss: 0.6343594193458557:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 114/151 [13:17<04:18,  6.98s/it]\u001b[A\n",
      "2|265|Loss: 0.6343594193458557:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 115/151 [13:24<04:11,  6.98s/it]\u001b[A\n",
      "2|266|Loss: 0.6451305747032166:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 115/151 [13:24<04:11,  6.98s/it]\u001b[A\n",
      "2|266|Loss: 0.6451305747032166:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 116/151 [13:31<04:04,  6.97s/it]\u001b[A\n",
      "2|267|Loss: 0.6292564272880554:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 116/151 [13:31<04:04,  6.97s/it]\u001b[A\n",
      "2|267|Loss: 0.6292564272880554:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 117/151 [13:38<03:57,  6.97s/it]\u001b[A\n",
      "2|268|Loss: 0.6286574602127075:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 117/151 [13:38<03:57,  6.97s/it]\u001b[A\n",
      "2|268|Loss: 0.6286574602127075:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 118/151 [13:45<03:50,  6.97s/it]\u001b[A\n",
      "2|269|Loss: 0.6331078410148621:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 118/151 [13:45<03:50,  6.97s/it]\u001b[A\n",
      "2|269|Loss: 0.6331078410148621:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 119/151 [13:52<03:43,  6.99s/it]\u001b[A\n",
      "2|270|Loss: 0.6260167956352234:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 119/151 [13:52<03:43,  6.99s/it]\u001b[A\n",
      "2|270|Loss: 0.6260167956352234:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 120/151 [13:59<03:36,  6.99s/it]\u001b[A\n",
      "2|271|Loss: 0.6387182474136353:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 120/151 [13:59<03:36,  6.99s/it]\u001b[A\n",
      "2|271|Loss: 0.6387182474136353:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 121/151 [14:06<03:29,  6.99s/it]\u001b[A\n",
      "2|272|Loss: 0.6352992653846741:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 121/151 [14:06<03:29,  6.99s/it]\u001b[A\n",
      "2|272|Loss: 0.6352992653846741:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 122/151 [14:13<03:22,  6.99s/it]\u001b[A\n",
      "2|273|Loss: 0.6405279040336609:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 122/151 [14:13<03:22,  6.99s/it]\u001b[A\n",
      "2|273|Loss: 0.6405279040336609:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 123/151 [14:20<03:15,  6.97s/it]\u001b[A\n",
      "2|274|Loss: 0.629018247127533:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 123/151 [14:20<03:15,  6.97s/it] \u001b[A\n",
      "2|274|Loss: 0.629018247127533:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 124/151 [14:27<03:08,  6.97s/it]\u001b[A\n",
      "2|275|Loss: 0.6294658780097961:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 124/151 [14:27<03:08,  6.97s/it]\u001b[A\n",
      "2|275|Loss: 0.6294658780097961:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 125/151 [14:33<03:01,  6.97s/it]\u001b[A\n",
      "2|276|Loss: 0.6290494799613953:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 125/151 [14:33<03:01,  6.97s/it]\u001b[A\n",
      "2|276|Loss: 0.6290494799613953:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 126/151 [14:40<02:54,  6.97s/it]\u001b[A\n",
      "2|277|Loss: 0.637048602104187:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 126/151 [14:40<02:54,  6.97s/it] \u001b[A\n",
      "2|277|Loss: 0.637048602104187:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 127/151 [14:47<02:47,  6.97s/it]\u001b[A\n",
      "2|278|Loss: 0.6366391777992249:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 127/151 [14:47<02:47,  6.97s/it]\u001b[A\n",
      "2|278|Loss: 0.6366391777992249:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 128/151 [14:54<02:40,  6.97s/it]\u001b[A\n",
      "2|279|Loss: 0.6382340788841248:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 128/151 [14:54<02:40,  6.97s/it]\u001b[A\n",
      "2|279|Loss: 0.6382340788841248:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 129/151 [15:01<02:33,  6.98s/it]\u001b[A\n",
      "2|280|Loss: 0.6345111727714539:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 129/151 [15:01<02:33,  6.98s/it]\u001b[A\n",
      "2|280|Loss: 0.6345111727714539:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 130/151 [15:08<02:26,  6.98s/it]\u001b[A\n",
      "2|281|Loss: 0.6327096819877625:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 130/151 [15:08<02:26,  6.98s/it]\u001b[A\n",
      "2|281|Loss: 0.6327096819877625:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 131/151 [15:15<02:19,  6.98s/it]\u001b[A\n",
      "2|282|Loss: 0.6290602087974548:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 131/151 [15:15<02:19,  6.98s/it]\u001b[A\n",
      "2|282|Loss: 0.6290602087974548:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 132/151 [15:22<02:12,  6.98s/it]\u001b[A\n",
      "2|283|Loss: 0.6166195273399353:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 132/151 [15:22<02:12,  6.98s/it]\u001b[A\n",
      "2|283|Loss: 0.6166195273399353:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 133/151 [15:29<02:05,  6.98s/it]\u001b[A\n",
      "2|284|Loss: 0.6269772052764893:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 133/151 [15:29<02:05,  6.98s/it]\u001b[A\n",
      "2|284|Loss: 0.6269772052764893:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 134/151 [15:36<01:58,  6.98s/it]\u001b[A\n",
      "2|285|Loss: 0.6256442070007324:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 134/151 [15:36<01:58,  6.98s/it]\u001b[A\n",
      "2|285|Loss: 0.6256442070007324:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 135/151 [15:43<01:51,  6.98s/it]\u001b[A\n",
      "2|286|Loss: 0.6429538726806641:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 135/151 [15:43<01:51,  6.98s/it]\u001b[A\n",
      "2|286|Loss: 0.6429538726806641:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 136/151 [15:50<01:44,  6.98s/it]\u001b[A\n",
      "2|287|Loss: 0.6384797096252441:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 136/151 [15:50<01:44,  6.98s/it]\u001b[A\n",
      "2|287|Loss: 0.6384797096252441:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 137/151 [15:57<01:37,  6.98s/it]\u001b[A\n",
      "2|288|Loss: 0.627886176109314:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 137/151 [15:57<01:37,  6.98s/it] \u001b[A\n",
      "2|288|Loss: 0.627886176109314:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 138/151 [16:04<01:30,  6.98s/it]\u001b[A\n",
      "2|289|Loss: 0.6308718323707581:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 138/151 [16:04<01:30,  6.98s/it]\u001b[A\n",
      "2|289|Loss: 0.6308718323707581:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 139/151 [16:11<01:23,  6.97s/it]\u001b[A\n",
      "2|290|Loss: 0.6285685300827026:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 139/151 [16:11<01:23,  6.97s/it]\u001b[A\n",
      "2|290|Loss: 0.6285685300827026:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 140/151 [16:18<01:16,  6.99s/it]\u001b[A\n",
      "2|291|Loss: 0.6417640447616577:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 140/151 [16:18<01:16,  6.99s/it]\u001b[A\n",
      "2|291|Loss: 0.6417640447616577:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 141/151 [16:25<01:09,  6.97s/it]\u001b[A\n",
      "2|292|Loss: 0.6292810440063477:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 141/151 [16:25<01:09,  6.97s/it]\u001b[A\n",
      "2|292|Loss: 0.6292810440063477:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 142/151 [16:32<01:02,  6.98s/it]\u001b[A\n",
      "2|293|Loss: 0.6279305219650269:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 142/151 [16:32<01:02,  6.98s/it]\u001b[A\n",
      "2|293|Loss: 0.6279305219650269:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 143/151 [16:39<00:55,  6.97s/it]\u001b[A\n",
      "2|294|Loss: 0.6312782764434814:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 143/151 [16:39<00:55,  6.97s/it]\u001b[A\n",
      "2|294|Loss: 0.6312782764434814:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 144/151 [16:46<00:48,  6.96s/it]\u001b[A\n",
      "2|295|Loss: 0.632046103477478:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 144/151 [16:46<00:48,  6.96s/it] \u001b[A\n",
      "2|295|Loss: 0.632046103477478:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 145/151 [16:53<00:41,  6.97s/it]\u001b[A\n",
      "2|296|Loss: 0.6378849744796753:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 145/151 [16:53<00:41,  6.97s/it]\u001b[A\n",
      "2|296|Loss: 0.6378849744796753:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 146/151 [17:00<00:34,  6.96s/it]\u001b[A\n",
      "2|297|Loss: 0.6285133957862854:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 146/151 [17:00<00:34,  6.96s/it]\u001b[A\n",
      "2|297|Loss: 0.6285133957862854:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 147/151 [17:07<00:27,  6.97s/it]\u001b[A\n",
      "2|298|Loss: 0.6346020102500916:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 147/151 [17:07<00:27,  6.97s/it]\u001b[A\n",
      "2|298|Loss: 0.6346020102500916:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 148/151 [17:14<00:20,  6.98s/it]\u001b[A\n",
      "2|299|Loss: 0.6310514211654663:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 148/151 [17:14<00:20,  6.98s/it]\u001b[A\n",
      "2|299|Loss: 0.6310514211654663:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 149/151 [17:21<00:13,  6.97s/it]\u001b[A\n",
      "2|300|Loss: 0.6208033561706543:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 149/151 [17:21<00:13,  6.97s/it]\u001b[A\n",
      "2|300|Loss: 0.6208033561706543:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 150/151 [17:28<00:06,  6.99s/it]\u001b[A\n",
      "2|301|Loss: 0.6438298225402832:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 150/151 [17:28<00:06,  6.99s/it]\u001b[A\n",
      "2|301|Loss: 0.6438298225402832: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 151/151 [17:35<00:00,  6.98s/it]\u001b[A\n",
      "2|302|Loss: 0.6262500286102295: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 151/151 [17:35<00:00,  6.98s/it]\u001b[AINFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...\n",
      "INFO:torchtune.utils._logging:Getting full model state dict took 19.40 secs\n",
      "INFO:torchtune.utils._logging:Retrieving optimizer state dict...\n",
      "INFO:torchtune.utils._logging:Getting optimizer state dict took 19.69 secs\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.98 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_5_20k-adapter-rev_1/hf_model_0001_1.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 5.00 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_5_20k-adapter-rev_1/hf_model_0002_1.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.92 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_5_20k-adapter-rev_1/hf_model_0003_1.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 1.17 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_5_20k-adapter-rev_1/hf_model_0004_1.pt\n",
      "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.04 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_5_20k-adapter-rev_1/adapter_1.pt\n",
      "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.04 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_5_20k-adapter-rev_1/adapter_model.bin\n",
      "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.00 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_5_20k-adapter-rev_1/adapter_config.json\n",
      "INFO:torchtune.utils._logging:Recipe checkpoint of size 0.08 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_5_20k-adapter-rev_1/recipe_state.pt\n",
      "INFO:torchtune.utils._logging:Saving checkpoint took 25.10 secs\n",
      "2|302|Loss: 0.6262500286102295: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 151/151 [18:22<00:00,  7.30s/it]\n",
      "3|453|Loss: 0.6191253662109375: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 151/151 [17:34<00:00,  6.99s/it]INFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...\n",
      "INFO:torchtune.utils._logging:Getting full model state dict took 10.25 secs\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.98 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_5_20k-adapter-rev_1/hf_model_0001_2.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 5.00 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_5_20k-adapter-rev_1/hf_model_0002_2.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.92 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_5_20k-adapter-rev_1/hf_model_0003_2.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 1.17 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_5_20k-adapter-rev_1/hf_model_0004_2.pt\n",
      "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.04 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_5_20k-adapter-rev_1/adapter_2.pt\n",
      "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.04 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_5_20k-adapter-rev_1/adapter_model.bin\n",
      "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.00 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_5_20k-adapter-rev_1/adapter_config.json\n",
      "INFO:torchtune.utils._logging:Saving final epoch checkpoint.\n",
      "INFO:torchtune.utils._logging:The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.\n",
      "INFO:torchtune.utils._logging:Saving checkpoint took 24.63 secs\n",
      "3|453|Loss: 0.6191253662109375: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 151/151 [18:11<00:00,  7.23s/it]\n",
      "wandb:                                                                                \n",
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:               global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "wandb:                      loss â–ˆâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "wandb:                        lr â–„â–„â–„â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–â–â–â–â–â–\n",
      "wandb:        peak_memory_active â–…â–ƒâ–„â–…â–†â–ˆâ–„â–†â–‚â–„â–‡â–ˆâ–‚â–…â–‡â–…â–†â–‚â–ˆâ–„â–â–‚â–‡â–‚â–…â–ƒâ–†â–ˆâ–ˆâ–…â–„â–ˆâ–ˆâ–…â–ƒâ–‡â–‚â–†â–„â–ˆ\n",
      "wandb:         peak_memory_alloc â–‡â–„â–‚â–…â–…â–†â–…â–…â–ˆâ–ˆâ–ƒâ–ƒâ–„â–‡â–‚â–„â–‚â–„â–ˆâ–ˆâ–ˆâ–†â–‡â–‚â–„â–â–ˆâ–…â–…â–‡â–â–†â–ˆâ–ˆâ–ˆâ–‚â–ˆâ–‡â–„â–…\n",
      "wandb:      peak_memory_reserved â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "wandb: tokens_per_second_per_gpu â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:               global_step 453\n",
      "wandb:                      loss 0.61913\n",
      "wandb:                        lr 0\n",
      "wandb:        peak_memory_active 39.08114\n",
      "wandb:         peak_memory_alloc 39.08114\n",
      "wandb:      peak_memory_reserved 68.70898\n",
      "wandb: tokens_per_second_per_gpu 1877.099\n",
      "wandb: \n",
      "wandb: ğŸš€ View run run-resume-critique-llama3_1_8b-tt_lora-model_5_20k-adapter-rev_1 at: https://wandb.ai/multimodalcompany/torchtune/runs/ihnhpkt7\n",
      "wandb: â­ï¸ View project at: https://wandb.ai/multimodalcompany/torchtune\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "wandb: Find logs at: logs/wandb/run-20241213_022947-ihnhpkt7/logs\n"
     ]
    }
   ],
   "source": [
    "%%pybash\n",
    "tune run \\\n",
    "    --nproc_per_node {NUM_GPUS} \\\n",
    "    lora_finetune_distributed \\\n",
    "    --config {CONFIG_FILE_PATH} \\\n",
    "    tokenizer.path={TOKENIZER_PATH} \\\n",
    "    checkpointer.checkpoint_dir={BASE_MODEL_PATH} \\\n",
    "    checkpointer.output_dir={OUTPUT_MODEL_PATH} \\\n",
    "    dataset.data_files={TRAINING_DATA_PATH} \\\n",
    "    metric_logger.group={WANDB_GROUP_NAME} \\\n",
    "    metric_logger.name={RUN_WANDB_NAME} \\\n",
    "\toutput_dir={OUTPUT_MODEL_PATH} \\\n",
    "\tmetric_logger.log_dir={LOGS_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
