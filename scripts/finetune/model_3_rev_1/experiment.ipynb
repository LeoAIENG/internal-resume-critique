{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 11 17:21:43 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   36C    P0              57W / 500W |      0MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          Off | 00000000:0C:00.0 Off |                    0 |\n",
      "| N/A   36C    P0              52W / 500W |      0MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-80GB          Off | 00000000:0D:00.0 Off |                    0 |\n",
      "| N/A   34C    P0              56W / 500W |      0MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-80GB          Off | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   36C    P0              55W / 500W |      0MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.5.1 (from -r requirements.txt (line 1))\n",
      "  Downloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting torchao==0.6.1 (from -r requirements.txt (line 2))\n",
      "  Downloading torchao-0.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting torchvision==0.20.1 (from -r requirements.txt (line 3))\n",
      "  Downloading torchvision-0.20.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting torchtune==0.4.0 (from -r requirements.txt (line 4))\n",
      "  Downloading torchtune-0.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting wandb==0.19.0 (from -r requirements.txt (line 5))\n",
      "  Downloading wandb-0.19.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting python-dotenv==1.0.1 (from -r requirements.txt (line 6))\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting filelock (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (4.11.0)\n",
      "Collecting networkx (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (3.1.4)\n",
      "Collecting fsspec (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.1.0 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (75.1.0)\n",
      "Collecting sympy==1.13.1 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting numpy (from torchvision==0.20.1->-r requirements.txt (line 3))\n",
      "  Downloading numpy-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.20.1->-r requirements.txt (line 3))\n",
      "  Downloading pillow-11.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting datasets (from torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting huggingface-hub (from torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors (from torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting sentencepiece (from torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tiktoken (from torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading tiktoken-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting blobfile>=2 (from torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading blobfile-3.0.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torchtune==0.4.0->-r requirements.txt (line 4)) (4.66.5)\n",
      "Collecting omegaconf (from torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torchtune==0.4.0->-r requirements.txt (line 4)) (5.9.0)\n",
      "Collecting click!=8.0.0,>=7.1 (from wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from wandb==0.19.0->-r requirements.txt (line 5)) (3.10.0)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 (from wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting pydantic<3,>=2.6 (from wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading pydantic-2.10.3-py3-none-any.whl.metadata (172 kB)\n",
      "Requirement already satisfied: pyyaml in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from wandb==0.19.0->-r requirements.txt (line 5)) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from wandb==0.19.0->-r requirements.txt (line 5)) (2.32.3)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading sentry_sdk-2.19.2-py2.py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting setproctitle (from wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading setproctitle-1.3.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting pycryptodomex>=3.8 (from blobfile>=2->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from blobfile>=2->torchtune==0.4.0->-r requirements.txt (line 4)) (2.2.3)\n",
      "Collecting lxml>=4.9 (from blobfile>=2->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading lxml-5.3.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb==0.19.0->-r requirements.txt (line 5)) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=2.6->wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.1 (from pydantic<3,>=2.6->wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading pydantic_core-2.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb==0.19.0->-r requirements.txt (line 5)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb==0.19.0->-r requirements.txt (line 5)) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb==0.19.0->-r requirements.txt (line 5)) (2024.8.30)\n",
      "Collecting pyarrow>=15.0.0 (from datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading pyarrow-18.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting xxhash (from datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading aiohttp-3.11.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from datasets->torchtune==0.4.0->-r requirements.txt (line 4)) (24.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from jinja2->torch==2.5.1->-r requirements.txt (line 1)) (2.1.3)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting regex>=2022.1.18 (from tiktoken->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets->torchtune==0.4.0->-r requirements.txt (line 4)) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading propcache-0.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from pandas->datasets->torchtune==0.4.0->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from pandas->datasets->torchtune==0.4.0->-r requirements.txt (line 4)) (2024.1)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Downloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl (906.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchao-0.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.20.1-cp312-cp312-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchtune-0.4.0-py3-none-any.whl (686 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m686.9/686.9 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wandb-0.19.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m137.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m153.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m178.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m119.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m165.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m136.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m157.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m148.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m134.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m144.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading blobfile-3.0.0-py3-none-any.whl (75 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Downloading pillow-11.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m154.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading pydantic-2.10.3-py3-none-any.whl (456 kB)\n",
      "Downloading pydantic_core-2.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m149.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-2.19.2-py2.py3-none-any.whl (322 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Downloading huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\n",
      "Downloading numpy-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m138.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m107.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Downloading safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (434 kB)\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
      "Downloading tiktoken-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading aiohttp-3.11.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Downloading lxml-5.3.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m133.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading pyarrow-18.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (40.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m168.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m153.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\n",
      "Downloading multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
      "Downloading propcache-0.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (243 kB)\n",
      "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Downloading yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (336 kB)\n",
      "Building wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=c0596918ad44c1f1caa5628bcd3c8043cc6cfc6326964f98489046ce772fbdb2\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/1f/be/48/13754633f1d08d1fbfc60d5e80ae1e5d7329500477685286cd\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: torchao, sentencepiece, mpmath, antlr4-python3-runtime, xxhash, tzdata, typing-extensions, sympy, smmap, setproctitle, sentry-sdk, safetensors, regex, python-dotenv, pycryptodomex, pyarrow, protobuf, propcache, pillow, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, lxml, fsspec, frozenlist, filelock, docker-pycreds, dill, click, annotated-types, aiohappyeyeballs, yarl, triton, tiktoken, pydantic-core, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, huggingface-hub, gitdb, blobfile, aiosignal, pydantic, nvidia-cusolver-cu12, gitpython, aiohttp, wandb, torch, torchvision, datasets, torchtune\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "Successfully installed aiohappyeyeballs-2.4.4 aiohttp-3.11.10 aiosignal-1.3.1 annotated-types-0.7.0 antlr4-python3-runtime-4.9.3 blobfile-3.0.0 click-8.1.7 datasets-3.2.0 dill-0.3.8 docker-pycreds-0.4.0 filelock-3.16.1 frozenlist-1.5.0 fsspec-2024.9.0 gitdb-4.0.11 gitpython-3.1.43 huggingface-hub-0.26.5 lxml-5.3.0 mpmath-1.3.0 multidict-6.1.0 multiprocess-0.70.16 networkx-3.4.2 numpy-2.2.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 omegaconf-2.3.0 pandas-2.2.3 pillow-11.0.0 propcache-0.2.1 protobuf-5.29.1 pyarrow-18.1.0 pycryptodomex-3.21.0 pydantic-2.10.3 pydantic-core-2.27.1 python-dotenv-1.0.1 regex-2024.11.6 safetensors-0.4.5 sentencepiece-0.2.0 sentry-sdk-2.19.2 setproctitle-1.3.4 smmap-5.0.1 sympy-1.13.1 tiktoken-0.8.0 torch-2.5.1 torchao-0.6.1 torchtune-0.4.0 torchvision-0.20.1 triton-3.1.0 typing-extensions-4.12.2 tzdata-2024.2 wandb-0.19.0 xxhash-3.5.0 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "ipython = get_ipython()\n",
    "@register_cell_magic\n",
    "def pybash(line, cell):\n",
    "    ipython.run_cell_magic('bash', '', cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONFIG\n",
    "NUM_GPUS = 4\n",
    "HF_TOKEN = os.environ[\"HF_TOKEN_R\"]\n",
    "IGNORE_PATTERNS = \"original/consolidated*\"\n",
    "CONFIG_FILE = \"llama_3_1_8b_lora_distributed.yaml\"\n",
    "\n",
    "## MODEL\n",
    "FT_MODEL_REPO = \"multimodalai\"\n",
    "BASE_MODEL_HF_ID = \"meta-llama/Llama-3.1-8B\"\n",
    "CLIENT = \"resume-critique\"\n",
    "MODEL = \"llama3_1_8b\"\n",
    "MODEL_NUMBER = \"4\"\n",
    "REV_N = \"1\"\n",
    "FT_METHOD = \"tt_lora\"\n",
    "MODEL_TYPE = \"adapter\"\n",
    "\n",
    "MDATA_ID = f\"model_{MODEL_NUMBER}_20k\"\n",
    "REV = f\"rev_{REV_N}\"\n",
    "FT_MODEL_NAME = f\"{CLIENT}-{MODEL}-{FT_METHOD}-{MDATA_ID}-{MODEL_TYPE}-{REV}\"\n",
    "FT_MODEL_HF_ID = f\"multimodalai/{FT_MODEL_NAME}\"\n",
    "\n",
    "## DATASET\n",
    "TRAINING_DATA = \"resume_critique_model_4.jsonl\"\n",
    "\n",
    "## PATH\n",
    "BASE_MODEL_PATH = \"base_model/\"\n",
    "TOKENIZER_PATH = f\"{BASE_MODEL_PATH}/original/tokenizer.model\"\n",
    "OUTPUT_MODEL_PATH = f\"checkpoint/{FT_MODEL_REPO}/{FT_MODEL_NAME}\"\n",
    "TRAINING_DATA_PATH = f\"data/{TRAINING_DATA}\"\n",
    "CONFIG_FILE_PATH = f\"config/{CONFIG_FILE}\"\n",
    "\n",
    "## TRACKING\n",
    "WANDB_GROUP_NAME = CLIENT\n",
    "RUN_WANDB_NAME = f\"run-{FT_MODEL_NAME}\"\n",
    "LOGS_PATH = \"logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {OUTPUT_MODEL_PATH}\n",
    "!mkdir -p {LOGS_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring files matching the following patterns: original/consolidated*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 16 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [01:58<00:00,  7.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded model repo and wrote to the following locations:\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_1_rev_3/base_model/special_tokens_map.json\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_1_rev_3/base_model/.gitattributes\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_1_rev_3/base_model/tokenizer.json\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_1_rev_3/base_model/LICENSE\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_1_rev_3/base_model/model.safetensors.index.json\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_1_rev_3/base_model/tokenizer_config.json\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_1_rev_3/base_model/model-00004-of-00004.safetensors\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_1_rev_3/base_model/README.md\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_1_rev_3/base_model/original\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_1_rev_3/base_model/USE_POLICY.md\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_1_rev_3/base_model/generation_config.json\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_1_rev_3/base_model/config.json\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_1_rev_3/base_model/model-00003-of-00004.safetensors\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_1_rev_3/base_model/.cache\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_1_rev_3/base_model/model-00001-of-00004.safetensors\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_1_rev_3/base_model/model-00002-of-00004.safetensors\n"
     ]
    }
   ],
   "source": [
    "%%pybash\n",
    "tune download {BASE_MODEL_HF_ID} --output-dir {BASE_MODEL_PATH} --ignore-patterns {IGNORE_PATTERNS} --hf-token {HF_TOKEN}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with torchrun...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1211 19:00:14.973000 48344 site-packages/torch/distributed/run.py:793] \n",
      "W1211 19:00:14.973000 48344 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "W1211 19:00:14.973000 48344 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1211 19:00:14.973000 48344 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "INFO:torchtune.utils._logging:Running LoRAFinetuneRecipeDistributed with resolved config:\n",
      "\n",
      "batch_size: 8\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: base_model/\n",
      "  checkpoint_files:\n",
      "  - model-00001-of-00004.safetensors\n",
      "  - model-00002-of-00004.safetensors\n",
      "  - model-00003-of-00004.safetensors\n",
      "  - model-00004-of-00004.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_1_20k-adapter-rev_2\n",
      "  recipe_checkpoint: null\n",
      "compile: false\n",
      "dataset:\n",
      "  _component_: torchtune.datasets.alpaca_dataset\n",
      "  data_files: data/resume_critique_model_1.jsonl\n",
      "  packed: false\n",
      "  source: json\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 4\n",
      "gradient_accumulation_steps: 4\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
      "lr_scheduler:\n",
      "  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup\n",
      "  num_warmup_steps: 30\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.WandBLogger\n",
      "  group: resume-critique\n",
      "  log_dir: logs/\n",
      "  name: run-resume-critique-llama3_1_8b-tt_lora-model_1_20k-adapter-rev_2\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1.lora_llama3_1_8b\n",
      "  apply_lora_to_mlp: true\n",
      "  apply_lora_to_output: true\n",
      "  lora_alpha: 16\n",
      "  lora_attn_modules:\n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  lora_dropout: 0.05\n",
      "  lora_rank: 8\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 0.0003\n",
      "  weight_decay: 0.01\n",
      "output_dir: checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_1_20k-adapter-rev_2\n",
      "profiler:\n",
      "  _component_: torchtune.training.setup_torch_profiler\n",
      "  active_steps: 2\n",
      "  cpu: true\n",
      "  cuda: true\n",
      "  enabled: false\n",
      "  num_cycles: 1\n",
      "  output_dir: checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_1_20k-adapter-rev_2/profiling_outputs\n",
      "  profile_memory: false\n",
      "  record_shapes: true\n",
      "  wait_steps: 5\n",
      "  warmup_steps: 3\n",
      "  with_flops: false\n",
      "  with_stack: false\n",
      "resume_from_checkpoint: false\n",
      "save_adapter_weights_only: false\n",
      "seed: 42\n",
      "shuffle: true\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.llama3.llama3_tokenizer\n",
      "  max_seq_len: 4096\n",
      "  path: base_model/original/tokenizer.model\n",
      "\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0\n",
      "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: leonardo-mm (multimodalcompany). Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.0\n",
      "wandb: Run data is saved locally in logs/wandb/run-20241211_190019-yv239bc3\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run run-resume-critique-llama3_1_8b-tt_lora-model_1_20k-adapter-rev_2\n",
      "wandb: â­ï¸ View project at https://wandb.ai/multimodalcompany/torchtune\n",
      "wandb: ğŸš€ View run at https://wandb.ai/multimodalcompany/torchtune/runs/yv239bc3\n",
      "INFO:torchtune.utils._logging:Logging base_model/torchtune_config.yaml to W&B under Files\n",
      "INFO:torchtune.utils._logging:FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n",
      "INFO:torchtune.utils._logging:Instantiating model and loading checkpoint took 4.74 secs\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 4.76 GiB\n",
      "\tGPU peak memory reserved: 4.86 GiB\n",
      "\tGPU peak memory active: 4.76 GiB\n",
      "INFO:torchtune.utils._logging:Optimizer is initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
      "INFO:torchtune.utils._logging:Learning rate scheduler is initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "1|151|Loss: 1.985501766204834: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 151/151 [1:29:18<00:00, 37.10s/it]INFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...\n",
      "INFO:torchtune.utils._logging:Getting full model state dict took 10.06 secs\n",
      "INFO:torchtune.utils._logging:Retrieving optimizer state dict...\n",
      "INFO:torchtune.utils._logging:Getting optimizer state dict took 10.28 secs\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.98 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_1_20k-adapter-rev_2/hf_model_0001_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 5.00 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_1_20k-adapter-rev_2/hf_model_0002_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.92 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_1_20k-adapter-rev_2/hf_model_0003_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 1.17 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_1_20k-adapter-rev_2/hf_model_0004_0.pt\n",
      "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.04 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_1_20k-adapter-rev_2/adapter_0.pt\n",
      "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.04 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_1_20k-adapter-rev_2/adapter_model.bin\n",
      "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.00 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_1_20k-adapter-rev_2/adapter_config.json\n",
      "INFO:torchtune.utils._logging:Recipe checkpoint of size 0.08 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_1_20k-adapter-rev_2/recipe_state.pt\n",
      "INFO:torchtune.utils._logging:Saving checkpoint took 48.16 secs\n",
      "\n",
      "1|151|Loss: 1.985501766204834: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 151/151 [1:30:21<00:00, 35.91s/it]\n",
      "\n",
      "  1%|          | 1/151 [00:24<1:00:08, 24.05s/it]\u001b[A\n",
      "2|152|Loss: 1.9346318244934082:   1%|          | 1/151 [00:24<1:00:08, 24.05s/it]\u001b[A\n",
      "2|152|Loss: 1.9346318244934082:   1%|â–         | 2/151 [00:56<1:12:21, 29.14s/it]\u001b[A\n",
      "2|153|Loss: 1.984307050704956:   1%|â–         | 2/151 [00:56<1:12:21, 29.14s/it] \u001b[A\n",
      "2|153|Loss: 1.984307050704956:   2%|â–         | 3/151 [01:31<1:18:30, 31.83s/it]\u001b[A\n",
      "2|154|Loss: 1.9406352043151855:   2%|â–         | 3/151 [01:31<1:18:30, 31.83s/it]\u001b[A\n",
      "2|154|Loss: 1.9406352043151855:   3%|â–         | 4/151 [02:12<1:26:39, 35.37s/it]\u001b[A\n",
      "2|155|Loss: 2.049086093902588:   3%|â–         | 4/151 [02:12<1:26:39, 35.37s/it] \u001b[A\n",
      "2|155|Loss: 2.049086093902588:   3%|â–         | 5/151 [02:51<1:28:47, 36.49s/it]\u001b[A\n",
      "2|156|Loss: 1.9886349439620972:   3%|â–         | 5/151 [02:51<1:28:47, 36.49s/it]\u001b[A\n",
      "2|156|Loss: 1.9886349439620972:   4%|â–         | 6/151 [03:28<1:28:52, 36.78s/it]\u001b[A\n",
      "2|157|Loss: 1.9500021934509277:   4%|â–         | 6/151 [03:28<1:28:52, 36.78s/it]\u001b[A\n",
      "2|157|Loss: 1.9500021934509277:   5%|â–         | 7/151 [04:04<1:27:57, 36.65s/it]\u001b[A\n",
      "2|158|Loss: 1.9335507154464722:   5%|â–         | 7/151 [04:04<1:27:57, 36.65s/it]\u001b[A\n",
      "2|158|Loss: 1.9335507154464722:   5%|â–Œ         | 8/151 [04:43<1:28:47, 37.25s/it]\u001b[A\n",
      "2|159|Loss: 1.9388548135757446:   5%|â–Œ         | 8/151 [04:43<1:28:47, 37.25s/it]\u001b[A\n",
      "2|159|Loss: 1.9388548135757446:   6%|â–Œ         | 9/151 [05:17<1:25:35, 36.17s/it]\u001b[A\n",
      "2|160|Loss: 1.9867743253707886:   6%|â–Œ         | 9/151 [05:17<1:25:35, 36.17s/it]\u001b[A\n",
      "2|160|Loss: 1.9867743253707886:   7%|â–‹         | 10/151 [05:50<1:23:16, 35.44s/it]\u001b[A\n",
      "2|161|Loss: 1.9648686647415161:   7%|â–‹         | 10/151 [05:50<1:23:16, 35.44s/it]\u001b[A\n",
      "2|161|Loss: 1.9648686647415161:   7%|â–‹         | 11/151 [06:27<1:23:52, 35.94s/it]\u001b[A\n",
      "2|162|Loss: 1.9712153673171997:   7%|â–‹         | 11/151 [06:27<1:23:52, 35.94s/it]\u001b[A\n",
      "2|162|Loss: 1.9712153673171997:   8%|â–Š         | 12/151 [07:06<1:25:14, 36.79s/it]\u001b[A\n",
      "2|163|Loss: 1.9953070878982544:   8%|â–Š         | 12/151 [07:06<1:25:14, 36.79s/it]\u001b[A\n",
      "2|163|Loss: 1.9953070878982544:   9%|â–Š         | 13/151 [07:43<1:24:29, 36.73s/it]\u001b[A\n",
      "2|164|Loss: 1.9571284055709839:   9%|â–Š         | 13/151 [07:43<1:24:29, 36.73s/it]\u001b[A\n",
      "2|164|Loss: 1.9571284055709839:   9%|â–‰         | 14/151 [08:24<1:26:39, 37.95s/it]\u001b[A\n",
      "2|165|Loss: 1.9864296913146973:   9%|â–‰         | 14/151 [08:24<1:26:39, 37.95s/it]\u001b[A\n",
      "2|165|Loss: 1.9864296913146973:  10%|â–‰         | 15/151 [09:00<1:24:52, 37.44s/it]\u001b[A\n",
      "2|166|Loss: 1.959677815437317:  10%|â–‰         | 15/151 [09:00<1:24:52, 37.44s/it] \u001b[A\n",
      "2|166|Loss: 1.959677815437317:  11%|â–ˆ         | 16/151 [09:34<1:21:51, 36.38s/it]\u001b[A\n",
      "2|167|Loss: 1.9284157752990723:  11%|â–ˆ         | 16/151 [09:34<1:21:51, 36.38s/it]\u001b[A\n",
      "2|167|Loss: 1.9284157752990723:  11%|â–ˆâ–        | 17/151 [10:03<1:16:37, 34.31s/it]\u001b[A\n",
      "2|168|Loss: 1.9160343408584595:  11%|â–ˆâ–        | 17/151 [10:03<1:16:37, 34.31s/it]\u001b[A\n",
      "2|168|Loss: 1.9160343408584595:  12%|â–ˆâ–        | 18/151 [10:43<1:19:27, 35.85s/it]\u001b[A\n",
      "2|169|Loss: 1.9472107887268066:  12%|â–ˆâ–        | 18/151 [10:43<1:19:27, 35.85s/it]\u001b[A\n",
      "2|169|Loss: 1.9472107887268066:  13%|â–ˆâ–        | 19/151 [11:17<1:17:36, 35.28s/it]\u001b[A\n",
      "2|170|Loss: 1.8700976371765137:  13%|â–ˆâ–        | 19/151 [11:17<1:17:36, 35.28s/it]\u001b[A\n",
      "2|170|Loss: 1.8700976371765137:  13%|â–ˆâ–        | 20/151 [11:48<1:14:23, 34.07s/it]\u001b[A\n",
      "2|171|Loss: 1.9234542846679688:  13%|â–ˆâ–        | 20/151 [11:48<1:14:23, 34.07s/it]\u001b[A\n",
      "2|171|Loss: 1.9234542846679688:  14%|â–ˆâ–        | 21/151 [12:19<1:11:58, 33.22s/it]\u001b[A\n",
      "2|172|Loss: 1.9316520690917969:  14%|â–ˆâ–        | 21/151 [12:19<1:11:58, 33.22s/it]\u001b[A\n",
      "2|172|Loss: 1.9316520690917969:  15%|â–ˆâ–        | 22/151 [12:48<1:08:31, 31.87s/it]\u001b[A\n",
      "2|173|Loss: 1.9624525308609009:  15%|â–ˆâ–        | 22/151 [12:48<1:08:31, 31.87s/it]\u001b[A\n",
      "2|173|Loss: 1.9624525308609009:  15%|â–ˆâ–Œ        | 23/151 [13:27<1:12:55, 34.18s/it]\u001b[A\n",
      "2|174|Loss: 1.958260416984558:  15%|â–ˆâ–Œ        | 23/151 [13:27<1:12:55, 34.18s/it] \u001b[A\n",
      "2|174|Loss: 1.958260416984558:  16%|â–ˆâ–Œ        | 24/151 [14:07<1:15:44, 35.78s/it]\u001b[A\n",
      "2|175|Loss: 1.9176582098007202:  16%|â–ˆâ–Œ        | 24/151 [14:07<1:15:44, 35.78s/it]\u001b[A\n",
      "2|175|Loss: 1.9176582098007202:  17%|â–ˆâ–‹        | 25/151 [14:45<1:16:34, 36.46s/it]\u001b[A\n",
      "2|176|Loss: 1.9708561897277832:  17%|â–ˆâ–‹        | 25/151 [14:45<1:16:34, 36.46s/it]\u001b[A\n",
      "2|176|Loss: 1.9708561897277832:  17%|â–ˆâ–‹        | 26/151 [15:26<1:18:41, 37.77s/it]\u001b[A\n",
      "2|177|Loss: 2.0148515701293945:  17%|â–ˆâ–‹        | 26/151 [15:26<1:18:41, 37.77s/it]\u001b[A\n",
      "2|177|Loss: 2.0148515701293945:  18%|â–ˆâ–Š        | 27/151 [16:02<1:17:14, 37.38s/it]\u001b[A\n",
      "2|178|Loss: 2.0010828971862793:  18%|â–ˆâ–Š        | 27/151 [16:02<1:17:14, 37.38s/it]\u001b[A\n",
      "2|178|Loss: 2.0010828971862793:  19%|â–ˆâ–Š        | 28/151 [16:42<1:18:20, 38.21s/it]\u001b[A\n",
      "2|179|Loss: 1.9398072957992554:  19%|â–ˆâ–Š        | 28/151 [16:42<1:18:20, 38.21s/it]\u001b[A\n",
      "2|179|Loss: 1.9398072957992554:  19%|â–ˆâ–‰        | 29/151 [17:11<1:11:51, 35.34s/it]\u001b[A\n",
      "2|180|Loss: 1.963008165359497:  19%|â–ˆâ–‰        | 29/151 [17:11<1:11:51, 35.34s/it] \u001b[A\n",
      "2|180|Loss: 1.963008165359497:  20%|â–ˆâ–‰        | 30/151 [17:45<1:10:29, 34.95s/it]\u001b[A\n",
      "2|181|Loss: 1.945568561553955:  20%|â–ˆâ–‰        | 30/151 [17:45<1:10:29, 34.95s/it]\u001b[A\n",
      "2|181|Loss: 1.945568561553955:  21%|â–ˆâ–ˆ        | 31/151 [18:23<1:11:26, 35.72s/it]\u001b[A\n",
      "2|182|Loss: 1.9002916812896729:  21%|â–ˆâ–ˆ        | 31/151 [18:23<1:11:26, 35.72s/it]\u001b[A\n",
      "2|182|Loss: 1.9002916812896729:  21%|â–ˆâ–ˆ        | 32/151 [19:02<1:12:54, 36.76s/it]\u001b[A\n",
      "2|183|Loss: 1.9092698097229004:  21%|â–ˆâ–ˆ        | 32/151 [19:02<1:12:54, 36.76s/it]\u001b[A\n",
      "2|183|Loss: 1.9092698097229004:  22%|â–ˆâ–ˆâ–       | 33/151 [19:34<1:09:51, 35.52s/it]\u001b[A\n",
      "2|184|Loss: 1.8900090456008911:  22%|â–ˆâ–ˆâ–       | 33/151 [19:34<1:09:51, 35.52s/it]\u001b[A\n",
      "2|184|Loss: 1.8900090456008911:  23%|â–ˆâ–ˆâ–       | 34/151 [20:12<1:10:19, 36.06s/it]\u001b[A\n",
      "2|185|Loss: 1.9808061122894287:  23%|â–ˆâ–ˆâ–       | 34/151 [20:12<1:10:19, 36.06s/it]\u001b[A\n",
      "2|185|Loss: 1.9808061122894287:  23%|â–ˆâ–ˆâ–       | 35/151 [20:50<1:10:44, 36.59s/it]\u001b[A\n",
      "2|186|Loss: 1.9300938844680786:  23%|â–ˆâ–ˆâ–       | 35/151 [20:50<1:10:44, 36.59s/it]\u001b[A\n",
      "2|186|Loss: 1.9300938844680786:  24%|â–ˆâ–ˆâ–       | 36/151 [21:16<1:04:26, 33.62s/it]\u001b[A\n",
      "2|187|Loss: 2.003101110458374:  24%|â–ˆâ–ˆâ–       | 36/151 [21:16<1:04:26, 33.62s/it] \u001b[A\n",
      "2|187|Loss: 2.003101110458374:  25%|â–ˆâ–ˆâ–       | 37/151 [21:52<1:04:50, 34.13s/it]\u001b[A\n",
      "2|188|Loss: 1.9716858863830566:  25%|â–ˆâ–ˆâ–       | 37/151 [21:52<1:04:50, 34.13s/it]\u001b[A\n",
      "2|188|Loss: 1.9716858863830566:  25%|â–ˆâ–ˆâ–Œ       | 38/151 [22:21<1:01:33, 32.68s/it]\u001b[A\n",
      "2|189|Loss: 1.983350157737732:  25%|â–ˆâ–ˆâ–Œ       | 38/151 [22:21<1:01:33, 32.68s/it] \u001b[A\n",
      "2|189|Loss: 1.983350157737732:  26%|â–ˆâ–ˆâ–Œ       | 39/151 [23:02<1:05:32, 35.11s/it]\u001b[A\n",
      "2|190|Loss: 1.9025689363479614:  26%|â–ˆâ–ˆâ–Œ       | 39/151 [23:02<1:05:32, 35.11s/it]\u001b[A\n",
      "2|190|Loss: 1.9025689363479614:  26%|â–ˆâ–ˆâ–‹       | 40/151 [23:37<1:05:14, 35.27s/it]\u001b[A\n",
      "2|191|Loss: 1.9333759546279907:  26%|â–ˆâ–ˆâ–‹       | 40/151 [23:37<1:05:14, 35.27s/it]\u001b[A\n",
      "2|191|Loss: 1.9333759546279907:  27%|â–ˆâ–ˆâ–‹       | 41/151 [24:16<1:06:27, 36.25s/it]\u001b[A\n",
      "2|192|Loss: 1.9337655305862427:  27%|â–ˆâ–ˆâ–‹       | 41/151 [24:16<1:06:27, 36.25s/it]\u001b[A\n",
      "2|192|Loss: 1.9337655305862427:  28%|â–ˆâ–ˆâ–Š       | 42/151 [24:51<1:05:27, 36.03s/it]\u001b[A\n",
      "2|193|Loss: 1.9852986335754395:  28%|â–ˆâ–ˆâ–Š       | 42/151 [24:51<1:05:27, 36.03s/it]\u001b[A\n",
      "2|193|Loss: 1.9852986335754395:  28%|â–ˆâ–ˆâ–Š       | 43/151 [25:32<1:07:19, 37.40s/it]\u001b[A\n",
      "2|194|Loss: 1.9354361295700073:  28%|â–ˆâ–ˆâ–Š       | 43/151 [25:32<1:07:19, 37.40s/it]\u001b[A\n",
      "2|194|Loss: 1.9354361295700073:  29%|â–ˆâ–ˆâ–‰       | 44/151 [25:57<59:59, 33.64s/it]  \u001b[A\n",
      "2|195|Loss: 1.9020824432373047:  29%|â–ˆâ–ˆâ–‰       | 44/151 [25:57<59:59, 33.64s/it]\u001b[A\n",
      "2|195|Loss: 1.9020824432373047:  30%|â–ˆâ–ˆâ–‰       | 45/151 [26:25<56:39, 32.07s/it]\u001b[A\n",
      "2|196|Loss: 1.959580659866333:  30%|â–ˆâ–ˆâ–‰       | 45/151 [26:25<56:39, 32.07s/it] \u001b[A\n",
      "2|196|Loss: 1.959580659866333:  30%|â–ˆâ–ˆâ–ˆ       | 46/151 [26:58<56:35, 32.34s/it]\u001b[A\n",
      "2|197|Loss: 1.9193753004074097:  30%|â–ˆâ–ˆâ–ˆ       | 46/151 [26:58<56:35, 32.34s/it]\u001b[A\n",
      "2|197|Loss: 1.9193753004074097:  31%|â–ˆâ–ˆâ–ˆ       | 47/151 [27:33<57:26, 33.14s/it]\u001b[A\n",
      "2|198|Loss: 1.9386059045791626:  31%|â–ˆâ–ˆâ–ˆ       | 47/151 [27:33<57:26, 33.14s/it]\u001b[A\n",
      "2|198|Loss: 1.9386059045791626:  32%|â–ˆâ–ˆâ–ˆâ–      | 48/151 [28:04<55:36, 32.39s/it]\u001b[A\n",
      "2|199|Loss: 1.8767399787902832:  32%|â–ˆâ–ˆâ–ˆâ–      | 48/151 [28:04<55:36, 32.39s/it]\u001b[A\n",
      "2|199|Loss: 1.8767399787902832:  32%|â–ˆâ–ˆâ–ˆâ–      | 49/151 [28:35<54:31, 32.08s/it]\u001b[A\n",
      "2|200|Loss: 1.9346847534179688:  32%|â–ˆâ–ˆâ–ˆâ–      | 49/151 [28:35<54:31, 32.08s/it]\u001b[A\n",
      "2|200|Loss: 1.9346847534179688:  33%|â–ˆâ–ˆâ–ˆâ–      | 50/151 [29:13<56:39, 33.66s/it]\u001b[A\n",
      "2|201|Loss: 1.909317135810852:  33%|â–ˆâ–ˆâ–ˆâ–      | 50/151 [29:13<56:39, 33.66s/it] \u001b[A\n",
      "2|201|Loss: 1.909317135810852:  34%|â–ˆâ–ˆâ–ˆâ–      | 51/151 [29:51<58:30, 35.11s/it]\u001b[A\n",
      "2|202|Loss: 1.9491978883743286:  34%|â–ˆâ–ˆâ–ˆâ–      | 51/151 [29:51<58:30, 35.11s/it]\u001b[A\n",
      "2|202|Loss: 1.9491978883743286:  34%|â–ˆâ–ˆâ–ˆâ–      | 52/151 [30:32<1:00:44, 36.82s/it]\u001b[A\n",
      "2|203|Loss: 1.9568263292312622:  34%|â–ˆâ–ˆâ–ˆâ–      | 52/151 [30:32<1:00:44, 36.82s/it]\u001b[A\n",
      "2|203|Loss: 1.9568263292312622:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 53/151 [31:08<59:50, 36.63s/it]  \u001b[A\n",
      "2|204|Loss: 1.927037000656128:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 53/151 [31:08<59:50, 36.63s/it] \u001b[A\n",
      "2|204|Loss: 1.927037000656128:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 54/151 [31:42<57:46, 35.74s/it]\u001b[A\n",
      "2|205|Loss: 1.9542208909988403:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 54/151 [31:42<57:46, 35.74s/it]\u001b[A\n",
      "2|205|Loss: 1.9542208909988403:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 55/151 [32:22<59:25, 37.14s/it]\u001b[A\n",
      "2|206|Loss: 1.8998658657073975:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 55/151 [32:22<59:25, 37.14s/it]\u001b[A\n",
      "2|206|Loss: 1.8998658657073975:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 56/151 [32:55<56:41, 35.81s/it]\u001b[A\n",
      "2|207|Loss: 1.9427577257156372:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 56/151 [32:55<56:41, 35.81s/it]\u001b[A\n",
      "2|207|Loss: 1.9427577257156372:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 57/151 [33:36<58:27, 37.31s/it]\u001b[A\n",
      "2|208|Loss: 1.9145135879516602:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 57/151 [33:36<58:27, 37.31s/it]\u001b[A\n",
      "2|208|Loss: 1.9145135879516602:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 58/151 [34:09<56:12, 36.26s/it]\u001b[A\n",
      "2|209|Loss: 1.9620615243911743:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 58/151 [34:09<56:12, 36.26s/it]\u001b[A\n",
      "2|209|Loss: 1.9620615243911743:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 59/151 [34:42<53:46, 35.07s/it]\u001b[A\n",
      "2|210|Loss: 1.853870153427124:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 59/151 [34:42<53:46, 35.07s/it] \u001b[A\n",
      "2|210|Loss: 1.853870153427124:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 60/151 [35:16<52:55, 34.89s/it]\u001b[A\n",
      "2|211|Loss: 1.9524130821228027:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 60/151 [35:16<52:55, 34.89s/it]\u001b[A\n",
      "2|211|Loss: 1.9524130821228027:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 61/151 [35:57<54:58, 36.65s/it]\u001b[A\n",
      "2|212|Loss: 1.879948377609253:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 61/151 [35:57<54:58, 36.65s/it] \u001b[A\n",
      "2|212|Loss: 1.879948377609253:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 62/151 [36:35<54:51, 36.98s/it]\u001b[A\n",
      "2|213|Loss: 1.9280375242233276:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 62/151 [36:35<54:51, 36.98s/it]\u001b[A\n",
      "2|213|Loss: 1.9280375242233276:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 63/151 [37:12<54:27, 37.13s/it]\u001b[A\n",
      "2|214|Loss: 2.028268575668335:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 63/151 [37:12<54:27, 37.13s/it] \u001b[A\n",
      "2|214|Loss: 2.028268575668335:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 64/151 [37:52<54:47, 37.78s/it]\u001b[A\n",
      "2|215|Loss: 1.8878663778305054:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 64/151 [37:52<54:47, 37.78s/it]\u001b[A\n",
      "2|215|Loss: 1.8878663778305054:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 65/151 [38:21<50:36, 35.31s/it]\u001b[A\n",
      "2|216|Loss: 1.9349299669265747:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 65/151 [38:21<50:36, 35.31s/it]\u001b[A\n",
      "2|216|Loss: 1.9349299669265747:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 66/151 [39:02<52:20, 36.95s/it]\u001b[A\n",
      "2|217|Loss: 1.9436380863189697:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 66/151 [39:02<52:20, 36.95s/it]\u001b[A\n",
      "2|217|Loss: 1.9436380863189697:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 67/151 [39:33<49:12, 35.15s/it]\u001b[A\n",
      "2|218|Loss: 1.99996817111969:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 67/151 [39:33<49:12, 35.15s/it]  \u001b[A\n",
      "2|218|Loss: 1.99996817111969:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 68/151 [40:09<49:00, 35.43s/it]\u001b[A\n",
      "2|219|Loss: 1.8791017532348633:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 68/151 [40:09<49:00, 35.43s/it]\u001b[A\n",
      "2|219|Loss: 1.8791017532348633:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 69/151 [40:36<45:12, 33.08s/it]\u001b[A\n",
      "2|220|Loss: 1.9177082777023315:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 69/151 [40:36<45:12, 33.08s/it]\u001b[A\n",
      "2|220|Loss: 1.9177082777023315:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 70/151 [41:17<47:47, 35.40s/it]\u001b[A\n",
      "2|221|Loss: 1.8923298120498657:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 70/151 [41:17<47:47, 35.40s/it]\u001b[A\n",
      "2|221|Loss: 1.8923298120498657:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 71/151 [41:58<49:21, 37.02s/it]\u001b[A\n",
      "2|222|Loss: 1.9477611780166626:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 71/151 [41:58<49:21, 37.02s/it]\u001b[A\n",
      "2|222|Loss: 1.9477611780166626:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 72/151 [42:39<50:14, 38.16s/it]\u001b[A\n",
      "2|223|Loss: 1.944686770439148:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 72/151 [42:39<50:14, 38.16s/it] \u001b[A\n",
      "2|223|Loss: 1.944686770439148:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 73/151 [43:18<50:00, 38.47s/it]\u001b[A\n",
      "2|224|Loss: 1.8893102407455444:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 73/151 [43:18<50:00, 38.47s/it]\u001b[A\n",
      "2|224|Loss: 1.8893102407455444:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 74/151 [43:52<47:39, 37.14s/it]\u001b[A\n",
      "2|225|Loss: 1.9394136667251587:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 74/151 [43:52<47:39, 37.14s/it]\u001b[A\n",
      "2|225|Loss: 1.9394136667251587:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 75/151 [44:31<47:45, 37.70s/it]\u001b[A\n",
      "2|226|Loss: 1.978822112083435:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 75/151 [44:31<47:45, 37.70s/it] \u001b[A\n",
      "2|226|Loss: 1.978822112083435:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 76/151 [45:04<45:16, 36.21s/it]\u001b[A\n",
      "2|227|Loss: 1.9581670761108398:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 76/151 [45:04<45:16, 36.21s/it]\u001b[A\n",
      "2|227|Loss: 1.9581670761108398:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 77/151 [45:39<44:12, 35.85s/it]\u001b[A\n",
      "2|228|Loss: 1.9665377140045166:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 77/151 [45:39<44:12, 35.85s/it]\u001b[A\n",
      "2|228|Loss: 1.9665377140045166:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78/151 [46:12<42:29, 34.93s/it]\u001b[A\n",
      "2|229|Loss: 1.8911267518997192:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78/151 [46:12<42:29, 34.93s/it]\u001b[A\n",
      "2|229|Loss: 1.8911267518997192:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 79/151 [46:47<42:10, 35.15s/it]\u001b[A\n",
      "2|230|Loss: 1.9027355909347534:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 79/151 [46:47<42:10, 35.15s/it]\u001b[A\n",
      "2|230|Loss: 1.9027355909347534:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 80/151 [47:18<39:50, 33.66s/it]\u001b[A\n",
      "2|231|Loss: 1.9172719717025757:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 80/151 [47:18<39:50, 33.66s/it]\u001b[A\n",
      "2|231|Loss: 1.9172719717025757:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 81/151 [47:58<41:46, 35.80s/it]\u001b[A\n",
      "2|232|Loss: 1.9060777425765991:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 81/151 [47:58<41:46, 35.80s/it]\u001b[A\n",
      "2|232|Loss: 1.9060777425765991:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 82/151 [48:39<42:53, 37.29s/it]\u001b[A\n",
      "2|233|Loss: 1.9583611488342285:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 82/151 [48:39<42:53, 37.29s/it]\u001b[A\n",
      "2|233|Loss: 1.9583611488342285:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 83/151 [49:20<43:24, 38.31s/it]\u001b[A\n",
      "2|234|Loss: 1.9927489757537842:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 83/151 [49:20<43:24, 38.31s/it]\u001b[A\n",
      "2|234|Loss: 1.9927489757537842:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 84/151 [49:55<41:53, 37.51s/it]\u001b[A\n",
      "2|235|Loss: 2.0062201023101807:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 84/151 [49:55<41:53, 37.51s/it]\u001b[A\n",
      "2|235|Loss: 2.0062201023101807:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 85/151 [50:33<41:22, 37.62s/it]\u001b[A\n",
      "2|236|Loss: 1.964532494544983:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 85/151 [50:33<41:22, 37.62s/it] \u001b[A\n",
      "2|236|Loss: 1.964532494544983:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 86/151 [50:56<35:53, 33.13s/it]\u001b[A\n",
      "2|237|Loss: 1.8794665336608887:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 86/151 [50:56<35:53, 33.13s/it]\u001b[A\n",
      "2|237|Loss: 1.8794665336608887:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 87/151 [51:31<35:54, 33.67s/it]\u001b[A\n",
      "2|238|Loss: 1.9781993627548218:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 87/151 [51:31<35:54, 33.67s/it]\u001b[A\n",
      "2|238|Loss: 1.9781993627548218:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 88/151 [52:00<33:55, 32.30s/it]\u001b[A\n",
      "2|239|Loss: 1.9323281049728394:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 88/151 [52:00<33:55, 32.30s/it]\u001b[A\n",
      "2|239|Loss: 1.9323281049728394:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 89/151 [52:36<34:36, 33.49s/it]\u001b[A\n",
      "2|240|Loss: 1.8837560415267944:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 89/151 [52:36<34:36, 33.49s/it]\u001b[A\n",
      "2|240|Loss: 1.8837560415267944:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 90/151 [53:10<34:06, 33.55s/it]\u001b[A\n",
      "2|241|Loss: 1.929884433746338:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 90/151 [53:10<34:06, 33.55s/it] \u001b[A\n",
      "2|241|Loss: 1.929884433746338:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 91/151 [53:47<34:29, 34.49s/it]\u001b[A\n",
      "2|242|Loss: 1.9828835725784302:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 91/151 [53:47<34:29, 34.49s/it]\u001b[A\n",
      "2|242|Loss: 1.9828835725784302:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 92/151 [54:24<34:50, 35.42s/it]\u001b[A\n",
      "2|243|Loss: 1.9191981554031372:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 92/151 [54:24<34:50, 35.42s/it]\u001b[A\n",
      "2|243|Loss: 1.9191981554031372:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 93/151 [54:56<33:14, 34.39s/it]\u001b[A\n",
      "2|244|Loss: 1.9506218433380127:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 93/151 [54:56<33:14, 34.39s/it]\u001b[A\n",
      "2|244|Loss: 1.9506218433380127:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 94/151 [55:37<34:29, 36.31s/it]\u001b[A\n",
      "2|245|Loss: 1.9262980222702026:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 94/151 [55:37<34:29, 36.31s/it]\u001b[A\n",
      "2|245|Loss: 1.9262980222702026:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 95/151 [56:14<34:02, 36.48s/it]\u001b[A\n",
      "2|246|Loss: 1.9821261167526245:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 95/151 [56:14<34:02, 36.48s/it]\u001b[A\n",
      "2|246|Loss: 1.9821261167526245:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 96/151 [56:44<31:39, 34.54s/it]\u001b[A\n",
      "2|247|Loss: 1.9469412565231323:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 96/151 [56:44<31:39, 34.54s/it]\u001b[A\n",
      "2|247|Loss: 1.9469412565231323:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 97/151 [57:18<30:55, 34.35s/it]\u001b[A\n",
      "2|248|Loss: 1.9162185192108154:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 97/151 [57:18<30:55, 34.35s/it]\u001b[A\n",
      "2|248|Loss: 1.9162185192108154:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 98/151 [57:53<30:27, 34.49s/it]\u001b[A\n",
      "2|249|Loss: 1.9712367057800293:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 98/151 [57:53<30:27, 34.49s/it]\u001b[A\n",
      "2|249|Loss: 1.9712367057800293:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 99/151 [58:24<29:10, 33.67s/it]\u001b[A\n",
      "2|250|Loss: 1.8723695278167725:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 99/151 [58:24<29:10, 33.67s/it]\u001b[A\n",
      "2|250|Loss: 1.8723695278167725:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 100/151 [59:04<30:11, 35.53s/it]\u001b[A\n",
      "2|251|Loss: 2.057792901992798:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 100/151 [59:04<30:11, 35.53s/it] \u001b[A\n",
      "2|251|Loss: 2.057792901992798:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 101/151 [59:43<30:21, 36.43s/it]\u001b[A\n",
      "2|252|Loss: 1.9914966821670532:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 101/151 [59:43<30:21, 36.43s/it]\u001b[A\n",
      "2|252|Loss: 1.9914966821670532:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 102/151 [1:00:19<29:49, 36.51s/it]\u001b[A\n",
      "2|253|Loss: 1.9488799571990967:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 102/151 [1:00:19<29:49, 36.51s/it]\u001b[A\n",
      "2|253|Loss: 1.9488799571990967:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 103/151 [1:00:55<29:00, 36.25s/it]\u001b[A\n",
      "2|254|Loss: 1.9543806314468384:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 103/151 [1:00:55<29:00, 36.25s/it]\u001b[A\n",
      "2|254|Loss: 1.9543806314468384:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 104/151 [1:01:31<28:22, 36.23s/it]\u001b[A\n",
      "2|255|Loss: 1.9739410877227783:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 104/151 [1:01:31<28:22, 36.23s/it]\u001b[A\n",
      "2|255|Loss: 1.9739410877227783:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 105/151 [1:02:07<27:46, 36.23s/it]\u001b[A\n",
      "2|256|Loss: 1.909650206565857:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 105/151 [1:02:07<27:46, 36.23s/it] \u001b[A\n",
      "2|256|Loss: 1.909650206565857:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 106/151 [1:02:41<26:27, 35.29s/it]\u001b[A\n",
      "2|257|Loss: 1.8984906673431396:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 106/151 [1:02:41<26:27, 35.29s/it]\u001b[A\n",
      "2|257|Loss: 1.8984906673431396:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 107/151 [1:03:14<25:25, 34.67s/it]\u001b[A\n",
      "2|258|Loss: 1.973992109298706:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 107/151 [1:03:14<25:25, 34.67s/it] \u001b[A\n",
      "2|258|Loss: 1.973992109298706:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 108/151 [1:03:49<24:56, 34.80s/it]\u001b[A\n",
      "2|259|Loss: 1.891539454460144:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 108/151 [1:03:49<24:56, 34.80s/it]\u001b[A\n",
      "2|259|Loss: 1.891539454460144:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 109/151 [1:04:30<25:37, 36.60s/it]\u001b[A\n",
      "2|260|Loss: 1.9932621717453003:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 109/151 [1:04:30<25:37, 36.60s/it]\u001b[A\n",
      "2|260|Loss: 1.9932621717453003:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 110/151 [1:05:09<25:38, 37.52s/it]\u001b[A\n",
      "2|261|Loss: 1.993125319480896:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 110/151 [1:05:09<25:38, 37.52s/it] \u001b[A\n",
      "2|261|Loss: 1.993125319480896:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 111/151 [1:05:49<25:24, 38.12s/it]\u001b[A\n",
      "2|262|Loss: 1.9678009748458862:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 111/151 [1:05:49<25:24, 38.12s/it]\u001b[A\n",
      "2|262|Loss: 1.9678009748458862:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 112/151 [1:06:29<25:04, 38.59s/it]\u001b[A\n",
      "2|263|Loss: 1.986527442932129:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 112/151 [1:06:29<25:04, 38.59s/it] \u001b[A\n",
      "2|263|Loss: 1.986527442932129:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 113/151 [1:06:59<22:57, 36.26s/it]\u001b[A\n",
      "2|264|Loss: 2.0124974250793457:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 113/151 [1:06:59<22:57, 36.26s/it]\u001b[A\n",
      "2|264|Loss: 2.0124974250793457:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 114/151 [1:07:35<22:14, 36.07s/it]\u001b[A\n",
      "2|265|Loss: 1.9217299222946167:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 114/151 [1:07:35<22:14, 36.07s/it]\u001b[A\n",
      "2|265|Loss: 1.9217299222946167:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 115/151 [1:08:09<21:15, 35.43s/it]\u001b[A\n",
      "2|266|Loss: 1.9083091020584106:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 115/151 [1:08:09<21:15, 35.43s/it]\u001b[A\n",
      "2|266|Loss: 1.9083091020584106:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 116/151 [1:08:39<19:43, 33.81s/it]\u001b[A\n",
      "2|267|Loss: 1.9438220262527466:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 116/151 [1:08:39<19:43, 33.81s/it]\u001b[A\n",
      "2|267|Loss: 1.9438220262527466:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 117/151 [1:09:08<18:24, 32.49s/it]\u001b[A\n",
      "2|268|Loss: 1.8540385961532593:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 117/151 [1:09:08<18:24, 32.49s/it]\u001b[A\n",
      "2|268|Loss: 1.8540385961532593:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 118/151 [1:09:39<17:35, 31.97s/it]\u001b[A\n",
      "2|269|Loss: 1.8750808238983154:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 118/151 [1:09:39<17:35, 31.97s/it]\u001b[A\n",
      "2|269|Loss: 1.8750808238983154:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 119/151 [1:10:09<16:43, 31.37s/it]\u001b[A\n",
      "2|270|Loss: 1.920924186706543:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 119/151 [1:10:09<16:43, 31.37s/it] \u001b[A\n",
      "2|270|Loss: 1.920924186706543:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 120/151 [1:10:42<16:23, 31.73s/it]\u001b[A\n",
      "2|271|Loss: 1.9774060249328613:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 120/151 [1:10:42<16:23, 31.73s/it]\u001b[A\n",
      "2|271|Loss: 1.9774060249328613:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 121/151 [1:11:22<17:13, 34.44s/it]\u001b[A\n",
      "2|272|Loss: 1.878352165222168:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 121/151 [1:11:22<17:13, 34.44s/it] \u001b[A\n",
      "2|272|Loss: 1.878352165222168:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 122/151 [1:11:57<16:38, 34.44s/it]\u001b[A\n",
      "2|273|Loss: 1.9887796640396118:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 122/151 [1:11:57<16:38, 34.44s/it]\u001b[A\n",
      "2|273|Loss: 1.9887796640396118:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 123/151 [1:12:30<15:52, 34.04s/it]\u001b[A\n",
      "2|274|Loss: 1.9103705883026123:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 123/151 [1:12:30<15:52, 34.04s/it]\u001b[A\n",
      "2|274|Loss: 1.9103705883026123:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 124/151 [1:12:57<14:18, 31.79s/it]\u001b[A\n",
      "2|275|Loss: 1.974022626876831:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 124/151 [1:12:57<14:18, 31.79s/it] \u001b[A\n",
      "2|275|Loss: 1.974022626876831:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 125/151 [1:13:34<14:28, 33.41s/it]\u001b[A\n",
      "2|276|Loss: 1.8914275169372559:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 125/151 [1:13:34<14:28, 33.41s/it]\u001b[A\n",
      "2|276|Loss: 1.8914275169372559:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 126/151 [1:14:12<14:34, 34.96s/it]\u001b[A\n",
      "2|277|Loss: 1.9469976425170898:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 126/151 [1:14:12<14:34, 34.96s/it]\u001b[A\n",
      "2|277|Loss: 1.9469976425170898:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 127/151 [1:14:44<13:34, 33.95s/it]\u001b[A\n",
      "2|278|Loss: 1.9442640542984009:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 127/151 [1:14:44<13:34, 33.95s/it]\u001b[A\n",
      "2|278|Loss: 1.9442640542984009:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 128/151 [1:15:23<13:35, 35.47s/it]\u001b[A\n",
      "2|279|Loss: 1.960225224494934:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 128/151 [1:15:23<13:35, 35.47s/it] \u001b[A\n",
      "2|279|Loss: 1.960225224494934:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 129/151 [1:16:01<13:17, 36.23s/it]\u001b[A\n",
      "2|280|Loss: 2.048074245452881:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 129/151 [1:16:01<13:17, 36.23s/it]\u001b[A\n",
      "2|280|Loss: 2.048074245452881:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 130/151 [1:16:39<12:50, 36.70s/it]\u001b[A\n",
      "2|281|Loss: 1.9448919296264648:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 130/151 [1:16:39<12:50, 36.70s/it]\u001b[A\n",
      "2|281|Loss: 1.9448919296264648:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 131/151 [1:17:16<12:14, 36.74s/it]\u001b[A\n",
      "2|282|Loss: 1.9380074739456177:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 131/151 [1:17:16<12:14, 36.74s/it]\u001b[A\n",
      "2|282|Loss: 1.9380074739456177:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 132/151 [1:17:51<11:29, 36.29s/it]\u001b[A\n",
      "2|283|Loss: 1.905200719833374:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 132/151 [1:17:51<11:29, 36.29s/it] \u001b[A\n",
      "2|283|Loss: 1.905200719833374:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 133/151 [1:18:28<10:57, 36.54s/it]\u001b[A\n",
      "2|284|Loss: 1.9141764640808105:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 133/151 [1:18:28<10:57, 36.54s/it]\u001b[A\n",
      "2|284|Loss: 1.9141764640808105:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 134/151 [1:19:05<10:22, 36.64s/it]\u001b[A\n",
      "2|285|Loss: 1.9042168855667114:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 134/151 [1:19:05<10:22, 36.64s/it]\u001b[A\n",
      "2|285|Loss: 1.9042168855667114:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 135/151 [1:19:39<09:32, 35.79s/it]\u001b[A\n",
      "2|286|Loss: 1.9239870309829712:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 135/151 [1:19:39<09:32, 35.79s/it]\u001b[A\n",
      "2|286|Loss: 1.9239870309829712:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 136/151 [1:20:13<08:52, 35.51s/it]\u001b[A\n",
      "2|287|Loss: 1.9465893507003784:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 136/151 [1:20:13<08:52, 35.51s/it]\u001b[A\n",
      "2|287|Loss: 1.9465893507003784:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 137/151 [1:20:52<08:31, 36.51s/it]\u001b[A\n",
      "2|288|Loss: 1.8973866701126099:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 137/151 [1:20:52<08:31, 36.51s/it]\u001b[A\n",
      "2|288|Loss: 1.8973866701126099:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 138/151 [1:21:25<07:41, 35.47s/it]\u001b[A\n",
      "2|289|Loss: 1.9466407299041748:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 138/151 [1:21:25<07:41, 35.47s/it]\u001b[A\n",
      "2|289|Loss: 1.9466407299041748:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 139/151 [1:21:57<06:51, 34.28s/it]\u001b[A\n",
      "2|290|Loss: 1.8889319896697998:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 139/151 [1:21:57<06:51, 34.28s/it]\u001b[A\n",
      "2|290|Loss: 1.8889319896697998:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 140/151 [1:22:33<06:21, 34.69s/it]\u001b[A\n",
      "2|291|Loss: 1.9703034162521362:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 140/151 [1:22:33<06:21, 34.69s/it]\u001b[A\n",
      "2|291|Loss: 1.9703034162521362:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 141/151 [1:23:13<06:03, 36.36s/it]\u001b[A\n",
      "2|292|Loss: 1.987313151359558:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 141/151 [1:23:13<06:03, 36.36s/it] \u001b[A\n",
      "2|292|Loss: 1.987313151359558:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 142/151 [1:23:46<05:18, 35.43s/it]\u001b[A\n",
      "2|293|Loss: 1.9189510345458984:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 142/151 [1:23:46<05:18, 35.43s/it]\u001b[A\n",
      "2|293|Loss: 1.9189510345458984:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 143/151 [1:24:21<04:41, 35.24s/it]\u001b[A\n",
      "2|294|Loss: 1.874039649963379:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 143/151 [1:24:21<04:41, 35.24s/it] \u001b[A\n",
      "2|294|Loss: 1.874039649963379:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 144/151 [1:24:57<04:08, 35.52s/it]\u001b[A\n",
      "2|295|Loss: 1.9746581315994263:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 144/151 [1:24:57<04:08, 35.52s/it]\u001b[A\n",
      "2|295|Loss: 1.9746581315994263:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 145/151 [1:25:24<03:17, 32.96s/it]\u001b[A\n",
      "2|296|Loss: 1.899817705154419:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 145/151 [1:25:24<03:17, 32.96s/it] \u001b[A\n",
      "2|296|Loss: 1.899817705154419:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 146/151 [1:26:01<02:50, 34.07s/it]\u001b[A\n",
      "2|297|Loss: 1.8668456077575684:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 146/151 [1:26:01<02:50, 34.07s/it]\u001b[A\n",
      "2|297|Loss: 1.8668456077575684:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 147/151 [1:26:35<02:16, 34.19s/it]\u001b[A\n",
      "2|298|Loss: 1.8895211219787598:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 147/151 [1:26:35<02:16, 34.19s/it]\u001b[A\n",
      "2|298|Loss: 1.8895211219787598:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 148/151 [1:27:10<01:43, 34.45s/it]\u001b[A\n",
      "2|299|Loss: 1.9473297595977783:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 148/151 [1:27:10<01:43, 34.45s/it]\u001b[A\n",
      "2|299|Loss: 1.9473297595977783:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 149/151 [1:27:47<01:10, 35.08s/it]\u001b[A\n",
      "2|300|Loss: 1.9452098608016968:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 149/151 [1:27:47<01:10, 35.08s/it]\u001b[A\n",
      "2|300|Loss: 1.9452098608016968:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 150/151 [1:28:22<00:35, 35.16s/it]\u001b[A\n",
      "2|301|Loss: 1.8804017305374146:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 150/151 [1:28:22<00:35, 35.16s/it]\u001b[A\n",
      "2|301|Loss: 1.8804017305374146: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 151/151 [1:29:03<00:00, 36.84s/it]\u001b[A\n",
      "2|302|Loss: 1.9629628658294678: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 151/151 [1:29:03<00:00, 36.84s/it]\u001b[AINFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...\n",
      "INFO:torchtune.utils._logging:Getting full model state dict took 10.53 secs\n",
      "INFO:torchtune.utils._logging:Retrieving optimizer state dict...\n",
      "INFO:torchtune.utils._logging:Getting optimizer state dict took 10.76 secs\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.98 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_1_20k-adapter-rev_2/hf_model_0001_1.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 5.00 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_1_20k-adapter-rev_2/hf_model_0002_1.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.92 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_1_20k-adapter-rev_2/hf_model_0003_1.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 1.17 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_1_20k-adapter-rev_2/hf_model_0004_1.pt\n",
      "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.04 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_1_20k-adapter-rev_2/adapter_1.pt\n",
      "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.04 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_1_20k-adapter-rev_2/adapter_model.bin\n",
      "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.00 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_1_20k-adapter-rev_2/adapter_config.json\n",
      "INFO:torchtune.utils._logging:Recipe checkpoint of size 0.08 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_1_20k-adapter-rev_2/recipe_state.pt\n",
      "INFO:torchtune.utils._logging:Saving checkpoint took 23.92 secs\n",
      "2|302|Loss: 1.9629628658294678: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 151/151 [1:29:45<00:00, 35.66s/it]\n",
      "3|453|Loss: 1.9155391454696655: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 151/151 [1:28:33<00:00, 36.07s/it]INFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...\n",
      "INFO:torchtune.utils._logging:Getting full model state dict took 20.74 secs\n",
      "INFO:torchtune.utils._logging:Retrieving optimizer state dict...\n",
      "INFO:torchtune.utils._logging:Getting optimizer state dict took 21.03 secs\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.98 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_1_20k-adapter-rev_2/hf_model_0001_2.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 5.00 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_1_20k-adapter-rev_2/hf_model_0002_2.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.92 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_1_20k-adapter-rev_2/hf_model_0003_2.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 1.17 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_1_20k-adapter-rev_2/hf_model_0004_2.pt\n",
      "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.04 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_1_20k-adapter-rev_2/adapter_2.pt\n",
      "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.04 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_1_20k-adapter-rev_2/adapter_model.bin\n",
      "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.00 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_1_20k-adapter-rev_2/adapter_config.json\n",
      "INFO:torchtune.utils._logging:Recipe checkpoint of size 0.08 GB saved to checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_1_20k-adapter-rev_2/recipe_state.pt\n",
      "INFO:torchtune.utils._logging:Saving checkpoint took 30.83 secs\n",
      "\n",
      "3|453|Loss: 1.9155391454696655: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 151/151 [1:29:34<00:00, 35.59s/it]\n",
      "\n",
      "  1%|          | 1/151 [00:32<1:21:48, 32.72s/it]\u001b[A\n",
      "4|454|Loss: 1.9393401145935059:   1%|          | 1/151 [00:32<1:21:48, 32.72s/it]\u001b[A\n",
      "4|454|Loss: 1.9393401145935059:   1%|â–         | 2/151 [01:13<1:33:00, 37.46s/it]\u001b[A\n",
      "4|455|Loss: 1.878541350364685:   1%|â–         | 2/151 [01:13<1:33:00, 37.46s/it] \u001b[A\n",
      "4|455|Loss: 1.878541350364685:   2%|â–         | 3/151 [01:50<1:31:19, 37.02s/it]\u001b[A\n",
      "4|456|Loss: 1.841317057609558:   2%|â–         | 3/151 [01:50<1:31:19, 37.02s/it]\u001b[A\n",
      "4|456|Loss: 1.841317057609558:   3%|â–         | 4/151 [02:26<1:29:58, 36.72s/it]\u001b[A\n",
      "4|457|Loss: 1.9111186265945435:   3%|â–         | 4/151 [02:26<1:29:58, 36.72s/it]\u001b[A\n",
      "4|457|Loss: 1.9111186265945435:   3%|â–         | 5/151 [03:06<1:32:38, 38.07s/it]\u001b[A\n",
      "4|458|Loss: 1.8791013956069946:   3%|â–         | 5/151 [03:06<1:32:38, 38.07s/it]\u001b[A\n",
      "4|458|Loss: 1.8791013956069946:   4%|â–         | 6/151 [03:38<1:27:07, 36.05s/it]\u001b[A\n",
      "4|459|Loss: 1.8661242723464966:   4%|â–         | 6/151 [03:38<1:27:07, 36.05s/it]\u001b[A\n",
      "4|459|Loss: 1.8661242723464966:   5%|â–         | 7/151 [04:19<1:30:15, 37.61s/it]\u001b[A\n",
      "4|460|Loss: 1.8727234601974487:   5%|â–         | 7/151 [04:19<1:30:15, 37.61s/it]\u001b[A\n",
      "4|460|Loss: 1.8727234601974487:   5%|â–Œ         | 8/151 [04:55<1:28:31, 37.15s/it]\u001b[A\n",
      "4|461|Loss: 1.8305624723434448:   5%|â–Œ         | 8/151 [04:55<1:28:31, 37.15s/it]\u001b[A\n",
      "4|461|Loss: 1.8305624723434448:   6%|â–Œ         | 9/151 [05:34<1:29:13, 37.70s/it]\u001b[A\n",
      "4|462|Loss: 1.868873119354248:   6%|â–Œ         | 9/151 [05:34<1:29:13, 37.70s/it] \u001b[A\n",
      "4|462|Loss: 1.868873119354248:   7%|â–‹         | 10/151 [06:04<1:22:46, 35.23s/it]\u001b[A\n",
      "4|463|Loss: 1.821881651878357:   7%|â–‹         | 10/151 [06:04<1:22:46, 35.23s/it]\u001b[A\n",
      "4|463|Loss: 1.821881651878357:   7%|â–‹         | 11/151 [06:42<1:24:19, 36.14s/it]\u001b[A\n",
      "4|464|Loss: 1.860756516456604:   7%|â–‹         | 11/151 [06:42<1:24:19, 36.14s/it]\u001b[A\n",
      "4|464|Loss: 1.860756516456604:   8%|â–Š         | 12/151 [07:14<1:21:01, 34.98s/it]\u001b[A\n",
      "4|465|Loss: 1.8583621978759766:   8%|â–Š         | 12/151 [07:14<1:21:01, 34.98s/it]\u001b[A\n",
      "4|465|Loss: 1.8583621978759766:   9%|â–Š         | 13/151 [07:50<1:20:51, 35.16s/it]\u001b[A\n",
      "4|466|Loss: 1.874603509902954:   9%|â–Š         | 13/151 [07:50<1:20:51, 35.16s/it] \u001b[A\n",
      "4|466|Loss: 1.874603509902954:   9%|â–‰         | 14/151 [08:20<1:16:38, 33.57s/it]\u001b[A\n",
      "4|467|Loss: 1.8862876892089844:   9%|â–‰         | 14/151 [08:20<1:16:38, 33.57s/it]\u001b[A\n",
      "4|467|Loss: 1.8862876892089844:  10%|â–‰         | 15/151 [08:52<1:15:20, 33.24s/it]\u001b[A\n",
      "4|468|Loss: 1.8489515781402588:  10%|â–‰         | 15/151 [08:52<1:15:20, 33.24s/it]\u001b[A\n",
      "4|468|Loss: 1.8489515781402588:  11%|â–ˆ         | 16/151 [09:33<1:19:53, 35.51s/it]\u001b[A\n",
      "4|469|Loss: 1.892999529838562:  11%|â–ˆ         | 16/151 [09:33<1:19:53, 35.51s/it] \u001b[A\n",
      "4|469|Loss: 1.892999529838562:  11%|â–ˆâ–        | 17/151 [10:09<1:19:14, 35.48s/it]\u001b[A\n",
      "4|470|Loss: 1.9392729997634888:  11%|â–ˆâ–        | 17/151 [10:09<1:19:14, 35.48s/it]\u001b[A\n",
      "4|470|Loss: 1.9392729997634888:  12%|â–ˆâ–        | 18/151 [10:42<1:17:29, 34.96s/it]\u001b[A\n",
      "4|471|Loss: 1.8266932964324951:  12%|â–ˆâ–        | 18/151 [10:42<1:17:29, 34.96s/it]\u001b[A\n",
      "4|471|Loss: 1.8266932964324951:  13%|â–ˆâ–        | 19/151 [11:16<1:15:49, 34.46s/it]\u001b[A\n",
      "4|472|Loss: 1.9154728651046753:  13%|â–ˆâ–        | 19/151 [11:16<1:15:49, 34.46s/it]\u001b[A\n",
      "4|472|Loss: 1.9154728651046753:  13%|â–ˆâ–        | 20/151 [11:54<1:17:53, 35.68s/it]\u001b[A\n",
      "4|473|Loss: 1.9543954133987427:  13%|â–ˆâ–        | 20/151 [11:54<1:17:53, 35.68s/it]\u001b[A\n",
      "4|473|Loss: 1.9543954133987427:  14%|â–ˆâ–        | 21/151 [12:23<1:12:52, 33.64s/it]\u001b[A\n",
      "4|474|Loss: 1.8726718425750732:  14%|â–ˆâ–        | 21/151 [12:23<1:12:52, 33.64s/it]\u001b[A\n",
      "4|474|Loss: 1.8726718425750732:  15%|â–ˆâ–        | 22/151 [13:00<1:14:33, 34.67s/it]\u001b[A\n",
      "4|475|Loss: 1.9108195304870605:  15%|â–ˆâ–        | 22/151 [13:00<1:14:33, 34.67s/it]\u001b[A\n",
      "4|475|Loss: 1.9108195304870605:  15%|â–ˆâ–Œ        | 23/151 [13:30<1:10:49, 33.20s/it]\u001b[A\n",
      "4|476|Loss: 1.9186019897460938:  15%|â–ˆâ–Œ        | 23/151 [13:30<1:10:49, 33.20s/it]\u001b[A\n",
      "4|476|Loss: 1.9186019897460938:  16%|â–ˆâ–Œ        | 24/151 [14:08<1:13:36, 34.77s/it]\u001b[A\n",
      "4|477|Loss: 1.877213954925537:  16%|â–ˆâ–Œ        | 24/151 [14:08<1:13:36, 34.77s/it] \u001b[A\n",
      "4|477|Loss: 1.877213954925537:  17%|â–ˆâ–‹        | 25/151 [14:48<1:16:10, 36.28s/it]\u001b[A\n",
      "4|478|Loss: 1.8298918008804321:  17%|â–ˆâ–‹        | 25/151 [14:48<1:16:10, 36.28s/it]\u001b[A\n",
      "4|478|Loss: 1.8298918008804321:  17%|â–ˆâ–‹        | 26/151 [15:22<1:14:10, 35.60s/it]\u001b[A\n",
      "4|479|Loss: 1.9479180574417114:  17%|â–ˆâ–‹        | 26/151 [15:22<1:14:10, 35.60s/it]\u001b[A\n",
      "4|479|Loss: 1.9479180574417114:  18%|â–ˆâ–Š        | 27/151 [15:58<1:13:56, 35.78s/it]\u001b[A\n",
      "4|480|Loss: 1.918487310409546:  18%|â–ˆâ–Š        | 27/151 [15:58<1:13:56, 35.78s/it] \u001b[A\n",
      "4|480|Loss: 1.918487310409546:  19%|â–ˆâ–Š        | 28/151 [16:36<1:14:16, 36.23s/it]\u001b[A\n",
      "4|481|Loss: 1.7896153926849365:  19%|â–ˆâ–Š        | 28/151 [16:36<1:14:16, 36.23s/it]\u001b[A\n",
      "4|481|Loss: 1.7896153926849365:  19%|â–ˆâ–‰        | 29/151 [17:07<1:10:59, 34.91s/it]\u001b[A\n",
      "4|482|Loss: 1.8872132301330566:  19%|â–ˆâ–‰        | 29/151 [17:07<1:10:59, 34.91s/it]\u001b[A\n",
      "4|482|Loss: 1.8872132301330566:  20%|â–ˆâ–‰        | 30/151 [17:46<1:12:38, 36.02s/it]\u001b[A\n",
      "4|483|Loss: 1.949133038520813:  20%|â–ˆâ–‰        | 30/151 [17:46<1:12:38, 36.02s/it] \u001b[A\n",
      "4|483|Loss: 1.949133038520813:  21%|â–ˆâ–ˆ        | 31/151 [18:27<1:14:43, 37.36s/it]\u001b[A\n",
      "4|484|Loss: 2.004932403564453:  21%|â–ˆâ–ˆ        | 31/151 [18:27<1:14:43, 37.36s/it]\u001b[A\n",
      "4|484|Loss: 2.004932403564453:  21%|â–ˆâ–ˆ        | 32/151 [19:02<1:12:49, 36.72s/it]\u001b[A\n",
      "4|485|Loss: 1.9494984149932861:  21%|â–ˆâ–ˆ        | 32/151 [19:02<1:12:49, 36.72s/it]\u001b[A\n",
      "4|485|Loss: 1.9494984149932861:  22%|â–ˆâ–ˆâ–       | 33/151 [19:41<1:13:34, 37.41s/it]\u001b[A\n",
      "4|486|Loss: 1.904818058013916:  22%|â–ˆâ–ˆâ–       | 33/151 [19:41<1:13:34, 37.41s/it] \u001b[A\n",
      "4|486|Loss: 1.904818058013916:  23%|â–ˆâ–ˆâ–       | 34/151 [20:20<1:14:12, 38.05s/it]\u001b[A\n",
      "4|487|Loss: 1.8453137874603271:  23%|â–ˆâ–ˆâ–       | 34/151 [20:20<1:14:12, 38.05s/it]\u001b[A\n",
      "4|487|Loss: 1.8453137874603271:  23%|â–ˆâ–ˆâ–       | 35/151 [20:56<1:12:26, 37.47s/it]\u001b[A\n",
      "4|488|Loss: 1.8233553171157837:  23%|â–ˆâ–ˆâ–       | 35/151 [20:56<1:12:26, 37.47s/it]\u001b[A\n",
      "4|488|Loss: 1.8233553171157837:  24%|â–ˆâ–ˆâ–       | 36/151 [21:21<1:04:29, 33.65s/it]\u001b[A\n",
      "4|489|Loss: 1.8150776624679565:  24%|â–ˆâ–ˆâ–       | 36/151 [21:21<1:04:29, 33.65s/it]\u001b[A\n",
      "4|489|Loss: 1.8150776624679565:  25%|â–ˆâ–ˆâ–       | 37/151 [21:53<1:02:48, 33.06s/it]\u001b[A\n",
      "4|490|Loss: 1.8904811143875122:  25%|â–ˆâ–ˆâ–       | 37/151 [21:53<1:02:48, 33.06s/it]\u001b[A\n",
      "4|490|Loss: 1.8904811143875122:  25%|â–ˆâ–ˆâ–Œ       | 38/151 [22:22<1:00:08, 31.93s/it]\u001b[A\n",
      "4|491|Loss: 1.9279552698135376:  25%|â–ˆâ–ˆâ–Œ       | 38/151 [22:22<1:00:08, 31.93s/it]\u001b[A\n",
      "4|491|Loss: 1.9279552698135376:  26%|â–ˆâ–ˆâ–Œ       | 39/151 [22:54<59:41, 31.97s/it]  \u001b[A\n",
      "4|492|Loss: 1.9559860229492188:  26%|â–ˆâ–ˆâ–Œ       | 39/151 [22:54<59:41, 31.97s/it]\u001b[A\n",
      "4|492|Loss: 1.9559860229492188:  26%|â–ˆâ–ˆâ–‹       | 40/151 [23:30<1:00:59, 32.97s/it]\u001b[A\n",
      "4|493|Loss: 1.8812527656555176:  26%|â–ˆâ–ˆâ–‹       | 40/151 [23:30<1:00:59, 32.97s/it]\u001b[A\n",
      "4|493|Loss: 1.8812527656555176:  27%|â–ˆâ–ˆâ–‹       | 41/151 [24:10<1:04:25, 35.14s/it]\u001b[A\n",
      "4|494|Loss: 1.8920150995254517:  27%|â–ˆâ–ˆâ–‹       | 41/151 [24:10<1:04:25, 35.14s/it]\u001b[A\n",
      "4|494|Loss: 1.8920150995254517:  28%|â–ˆâ–ˆâ–Š       | 42/151 [24:48<1:05:19, 35.95s/it]\u001b[A\n",
      "4|495|Loss: 1.9672839641571045:  28%|â–ˆâ–ˆâ–Š       | 42/151 [24:48<1:05:19, 35.95s/it]\u001b[A\n",
      "4|495|Loss: 1.9672839641571045:  28%|â–ˆâ–ˆâ–Š       | 43/151 [25:28<1:07:19, 37.40s/it]\u001b[A\n",
      "4|496|Loss: 1.9937552213668823:  28%|â–ˆâ–ˆâ–Š       | 43/151 [25:28<1:07:19, 37.40s/it]\u001b[A\n",
      "4|496|Loss: 1.9937552213668823:  29%|â–ˆâ–ˆâ–‰       | 44/151 [25:59<1:02:49, 35.23s/it]\u001b[A\n",
      "4|497|Loss: 1.89546799659729:  29%|â–ˆâ–ˆâ–‰       | 44/151 [25:59<1:02:49, 35.23s/it]  \u001b[A\n",
      "4|497|Loss: 1.89546799659729:  30%|â–ˆâ–ˆâ–‰       | 45/151 [26:33<1:01:55, 35.05s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "%%pybash\n",
    "tune run \\\n",
    "    --nproc_per_node {NUM_GPUS} \\\n",
    "    lora_finetune_distributed \\\n",
    "    --config {CONFIG_FILE_PATH} \\\n",
    "    tokenizer.path={TOKENIZER_PATH} \\\n",
    "    checkpointer.checkpoint_dir={BASE_MODEL_PATH} \\\n",
    "    checkpointer.output_dir={OUTPUT_MODEL_PATH} \\\n",
    "    dataset.data_files={TRAINING_DATA_PATH} \\\n",
    "    metric_logger.group={WANDB_GROUP_NAME} \\\n",
    "    metric_logger.name={RUN_WANDB_NAME} \\\n",
    "\toutput_dir={OUTPUT_MODEL_PATH} \\\n",
    "\tmetric_logger.log_dir={LOGS_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
