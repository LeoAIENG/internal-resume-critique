{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec 13 01:17:04 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   0  NVIDIA A40                     Off | 00000000:0B:00.0 Off |                    0 |\n",
      "|  0%   29C    P8              21W / 300W |      0MiB / 46068MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A40                     Off | 00000000:0C:00.0 Off |                    0 |\n",
      "|  0%   29C    P8              21W / 300W |      0MiB / 46068MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A40                     Off | 00000000:0D:00.0 Off |                    0 |\n",
      "|  0%   29C    P8              21W / 300W |      0MiB / 46068MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A40                     Off | 00000000:0E:00.0 Off |                    0 |\n",
      "|  0%   33C    P8              21W / 300W |      0MiB / 46068MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.5.1 (from -r requirements.txt (line 1))\n",
      "  Downloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting torchao==0.6.1 (from -r requirements.txt (line 2))\n",
      "  Downloading torchao-0.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting torchvision==0.20.1 (from -r requirements.txt (line 3))\n",
      "  Downloading torchvision-0.20.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting torchtune==0.4.0 (from -r requirements.txt (line 4))\n",
      "  Downloading torchtune-0.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting wandb==0.19.0 (from -r requirements.txt (line 5))\n",
      "  Downloading wandb-0.19.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting python-dotenv==1.0.1 (from -r requirements.txt (line 6))\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting filelock (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (4.11.0)\n",
      "Collecting networkx (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (3.1.4)\n",
      "Collecting fsspec (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.1.0 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (75.1.0)\n",
      "Collecting sympy==1.13.1 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting numpy (from torchvision==0.20.1->-r requirements.txt (line 3))\n",
      "  Downloading numpy-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.20.1->-r requirements.txt (line 3))\n",
      "  Downloading pillow-11.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting datasets (from torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting huggingface-hub (from torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors (from torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting sentencepiece (from torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tiktoken (from torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading tiktoken-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting blobfile>=2 (from torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading blobfile-3.0.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torchtune==0.4.0->-r requirements.txt (line 4)) (4.66.5)\n",
      "Collecting omegaconf (from torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from torchtune==0.4.0->-r requirements.txt (line 4)) (5.9.0)\n",
      "Collecting click!=8.0.0,>=7.1 (from wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from wandb==0.19.0->-r requirements.txt (line 5)) (3.10.0)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 (from wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting pydantic<3,>=2.6 (from wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading pydantic-2.10.3-py3-none-any.whl.metadata (172 kB)\n",
      "Requirement already satisfied: pyyaml in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from wandb==0.19.0->-r requirements.txt (line 5)) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from wandb==0.19.0->-r requirements.txt (line 5)) (2.32.3)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading sentry_sdk-2.19.2-py2.py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting setproctitle (from wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading setproctitle-1.3.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting pycryptodomex>=3.8 (from blobfile>=2->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from blobfile>=2->torchtune==0.4.0->-r requirements.txt (line 4)) (2.2.3)\n",
      "Collecting lxml>=4.9 (from blobfile>=2->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading lxml-5.3.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb==0.19.0->-r requirements.txt (line 5)) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=2.6->wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.1 (from pydantic<3,>=2.6->wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading pydantic_core-2.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb==0.19.0->-r requirements.txt (line 5)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb==0.19.0->-r requirements.txt (line 5)) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb==0.19.0->-r requirements.txt (line 5)) (2024.8.30)\n",
      "Collecting pyarrow>=15.0.0 (from datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading pyarrow-18.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting xxhash (from datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec (from torch==2.5.1->-r requirements.txt (line 1))\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading aiohttp-3.11.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from datasets->torchtune==0.4.0->-r requirements.txt (line 4)) (24.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from jinja2->torch==2.5.1->-r requirements.txt (line 1)) (2.1.3)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting regex>=2022.1.18 (from tiktoken->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets->torchtune==0.4.0->-r requirements.txt (line 4)) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading propcache-0.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb==0.19.0->-r requirements.txt (line 5))\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from pandas->datasets->torchtune==0.4.0->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/miniconda3/lib/python3.12/site-packages (from pandas->datasets->torchtune==0.4.0->-r requirements.txt (line 4)) (2024.1)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets->torchtune==0.4.0->-r requirements.txt (line 4))\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Downloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl (906.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchao-0.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.20.1-cp312-cp312-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchtune-0.4.0-py3-none-any.whl (686 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m686.9/686.9 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wandb-0.19.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m122.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m115.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading blobfile-3.0.0-py3-none-any.whl (75 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Downloading pillow-11.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading pydantic-2.10.3-py3-none-any.whl (456 kB)\n",
      "Downloading pydantic_core-2.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-2.19.2-py2.py3-none-any.whl (322 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Downloading huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\n",
      "Downloading numpy-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Downloading safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (434 kB)\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
      "Downloading tiktoken-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading aiohttp-3.11.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Downloading lxml-5.3.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading pyarrow-18.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (40.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\n",
      "Downloading multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
      "Downloading propcache-0.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (243 kB)\n",
      "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Downloading yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (336 kB)\n",
      "Building wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=c70896ace1de830e24f6e79ffd8453f252be04c91c084051f9639fabfd0a4364\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/1f/be/48/13754633f1d08d1fbfc60d5e80ae1e5d7329500477685286cd\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: torchao, sentencepiece, mpmath, antlr4-python3-runtime, xxhash, tzdata, typing-extensions, sympy, smmap, setproctitle, sentry-sdk, safetensors, regex, python-dotenv, pycryptodomex, pyarrow, protobuf, propcache, pillow, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, lxml, fsspec, frozenlist, filelock, docker-pycreds, dill, click, annotated-types, aiohappyeyeballs, yarl, triton, tiktoken, pydantic-core, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, huggingface-hub, gitdb, blobfile, aiosignal, pydantic, nvidia-cusolver-cu12, gitpython, aiohttp, wandb, torch, torchvision, datasets, torchtune\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "Successfully installed aiohappyeyeballs-2.4.4 aiohttp-3.11.10 aiosignal-1.3.1 annotated-types-0.7.0 antlr4-python3-runtime-4.9.3 blobfile-3.0.0 click-8.1.7 datasets-3.2.0 dill-0.3.8 docker-pycreds-0.4.0 filelock-3.16.1 frozenlist-1.5.0 fsspec-2024.9.0 gitdb-4.0.11 gitpython-3.1.43 huggingface-hub-0.26.5 lxml-5.3.0 mpmath-1.3.0 multidict-6.1.0 multiprocess-0.70.16 networkx-3.4.2 numpy-2.2.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 omegaconf-2.3.0 pandas-2.2.3 pillow-11.0.0 propcache-0.2.1 protobuf-5.29.1 pyarrow-18.1.0 pycryptodomex-3.21.0 pydantic-2.10.3 pydantic-core-2.27.1 python-dotenv-1.0.1 regex-2024.11.6 safetensors-0.4.5 sentencepiece-0.2.0 sentry-sdk-2.19.2 setproctitle-1.3.4 smmap-5.0.1 sympy-1.13.1 tiktoken-0.8.0 torch-2.5.1 torchao-0.6.1 torchtune-0.4.0 torchvision-0.20.1 triton-3.1.0 typing-extensions-4.12.2 tzdata-2024.2 wandb-0.19.0 xxhash-3.5.0 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "ipython = get_ipython()\n",
    "@register_cell_magic\n",
    "def pybash(line, cell):\n",
    "    ipython.run_cell_magic('bash', '', cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONFIG\n",
    "NUM_GPUS = 4\n",
    "HF_TOKEN = os.environ[\"HF_TOKEN_R\"]\n",
    "IGNORE_PATTERNS = \"original/consolidated*\"\n",
    "CONFIG_FILE = \"llama_3_1_8b_lora_distributed_resume.yaml\"\n",
    "\n",
    "## MODEL\n",
    "FT_MODEL_REPO = \"multimodalai\"\n",
    "BASE_MODEL_HF_ID = \"meta-llama/Llama-3.1-8B\"\n",
    "CLIENT = \"resume-critique\"\n",
    "MODEL = \"llama3_1_8b\"\n",
    "MODEL_NUMBER = \"2\"\n",
    "REV_N = \"1\"\n",
    "FT_METHOD = \"tt_lora\"\n",
    "MODEL_TYPE = \"adapter\"\n",
    "\n",
    "MDATA_ID = f\"model_{MODEL_NUMBER}_20k\"\n",
    "REV = f\"rev_{REV_N}\"\n",
    "FT_MODEL_NAME = f\"{CLIENT}-{MODEL}-{FT_METHOD}-{MDATA_ID}-{MODEL_TYPE}-{REV}\"\n",
    "FT_MODEL_HF_ID = f\"multimodalai/{FT_MODEL_NAME}\"\n",
    "\n",
    "## DATASET\n",
    "TRAINING_DATA = \"resume_critique_model_2.jsonl\"\n",
    "\n",
    "## PATH\n",
    "BASE_MODEL_PATH = \"base_model/\"\n",
    "TOKENIZER_PATH = f\"{BASE_MODEL_PATH}/original/tokenizer.model\"\n",
    "OUTPUT_MODEL_PATH = f\"checkpoint/{FT_MODEL_REPO}/{FT_MODEL_NAME}\"\n",
    "TRAINING_DATA_PATH = f\"data/{TRAINING_DATA}\"\n",
    "CONFIG_FILE_PATH = f\"config/{CONFIG_FILE}\"\n",
    "\n",
    "## TRACKING\n",
    "WANDB_GROUP_NAME = CLIENT\n",
    "RUN_WANDB_NAME = f\"run-{FT_MODEL_NAME}\"\n",
    "LOGS_PATH = \"logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {OUTPUT_MODEL_PATH}\n",
    "!mkdir -p {LOGS_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring files matching the following patterns: original/consolidated*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 16 files: 100%|██████████| 16/16 [01:59<00:00,  7.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded model repo and wrote to the following locations:\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_2_rev_1/base_model/special_tokens_map.json\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_2_rev_1/base_model/.gitattributes\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_2_rev_1/base_model/tokenizer.json\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_2_rev_1/base_model/LICENSE\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_2_rev_1/base_model/model.safetensors.index.json\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_2_rev_1/base_model/tokenizer_config.json\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_2_rev_1/base_model/model-00004-of-00004.safetensors\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_2_rev_1/base_model/README.md\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_2_rev_1/base_model/original\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_2_rev_1/base_model/USE_POLICY.md\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_2_rev_1/base_model/generation_config.json\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_2_rev_1/base_model/config.json\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_2_rev_1/base_model/model-00003-of-00004.safetensors\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_2_rev_1/base_model/.cache\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_2_rev_1/base_model/model-00001-of-00004.safetensors\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_2_rev_1/base_model/model-00002-of-00004.safetensors\n"
     ]
    }
   ],
   "source": [
    "%%pybash\n",
    "tune download {BASE_MODEL_HF_ID} --output-dir {BASE_MODEL_PATH} --ignore-patterns {IGNORE_PATTERNS} --hf-token {HF_TOKEN}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with torchrun...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1213 01:25:26.200000 11482 site-packages/torch/distributed/run.py:793] \n",
      "W1213 01:25:26.200000 11482 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "W1213 01:25:26.200000 11482 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1213 01:25:26.200000 11482 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda3/bin/tune\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.12/site-packages/torchtune/_cli/tune.py\", line 49, in main\n",
      "    parser.run(args)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.12/site-packages/torchtune/_cli/tune.py\", line 43, in run\n",
      "    args.func(args)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.12/site-packages/torchtune/_cli/run.py\", line 206, in _run_cmd\n",
      "    self._run_distributed(args, is_builtin=is_builtin)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.12/site-packages/torchtune/_cli/run.py\", line 95, in _run_distributed\n",
      "    run(args)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.12/site-packages/torch/distributed/run.py\", line 910, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.12/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.12/site-packages/torch/distributed/launcher/api.py\", line 260, in launch_agent\n",
      "    result = agent.run()\n",
      "             ^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.12/site-packages/torch/distributed/elastic/metrics/api.py\", line 137, in wrapper\n",
      "    result = f(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py\", line 696, in run\n",
      "    result = self._invoke_run(role)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py\", line 849, in _invoke_run\n",
      "    self._initialize_workers(self._worker_group)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.12/site-packages/torch/distributed/elastic/metrics/api.py\", line 137, in wrapper\n",
      "    result = f(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py\", line 668, in _initialize_workers\n",
      "    self._rendezvous(worker_group)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.12/site-packages/torch/distributed/elastic/metrics/api.py\", line 137, in wrapper\n",
      "    result = f(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py\", line 500, in _rendezvous\n",
      "    rdzv_info = spec.rdzv_handler.next_rendezvous()\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.12/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\", line 67, in next_rendezvous\n",
      "    self._store = TCPStore(  # type: ignore[call-arg]\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: The server socket has failed to listen on any local network address. port: 29500, useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'tune run \\\\\\n    --nproc_per_node 4 \\\\\\n    lora_finetune_distributed \\\\\\n    --config config/llama_3_1_8b_lora_distributed.yaml \\\\\\n    tokenizer.path=base_model//original/tokenizer.model \\\\\\n    checkpointer.checkpoint_dir=base_model/ \\\\\\n    checkpointer.output_dir=checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_2_20k-adapter-rev_1 \\\\\\n    dataset.data_files=data/resume_critique_model_2.jsonl \\\\\\n    metric_logger.group=resume-critique \\\\\\n    metric_logger.name=run-resume-critique-llama3_1_8b-tt_lora-model_2_20k-adapter-rev_1 \\\\\\n\\toutput_dir=checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_2_20k-adapter-rev_1 \\\\\\n\\tmetric_logger.log_dir=logs/\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_cell_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpybash\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtune run \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    --nproc_per_node \u001b[39m\u001b[38;5;132;01m{NUM_GPUS}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    lora_finetune_distributed \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    --config \u001b[39m\u001b[38;5;132;01m{CONFIG_FILE_PATH}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    tokenizer.path=\u001b[39m\u001b[38;5;132;01m{TOKENIZER_PATH}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    checkpointer.checkpoint_dir=\u001b[39m\u001b[38;5;132;01m{BASE_MODEL_PATH}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    checkpointer.output_dir=\u001b[39m\u001b[38;5;132;01m{OUTPUT_MODEL_PATH}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    dataset.data_files=\u001b[39m\u001b[38;5;132;01m{TRAINING_DATA_PATH}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    metric_logger.group=\u001b[39m\u001b[38;5;132;01m{WANDB_GROUP_NAME}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    metric_logger.name=\u001b[39m\u001b[38;5;132;01m{RUN_WANDB_NAME}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124moutput_dir=\u001b[39m\u001b[38;5;132;01m{OUTPUT_MODEL_PATH}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mmetric_logger.log_dir=\u001b[39m\u001b[38;5;132;01m{LOGS_PATH}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2541\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2540\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2541\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m, in \u001b[0;36mpybash\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;129m@register_cell_magic\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpybash\u001b[39m(line, cell):\n\u001b[0;32m----> 7\u001b[0m     ipython\u001b[38;5;241m.\u001b[39mrun_cell_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbash\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, cell\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mglobals\u001b[39m()))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2541\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2540\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2541\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/IPython/core/magics/script.py:155\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshebang(line, cell)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/IPython/core/magics/script.py:315\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'tune run \\\\\\n    --nproc_per_node 4 \\\\\\n    lora_finetune_distributed \\\\\\n    --config config/llama_3_1_8b_lora_distributed.yaml \\\\\\n    tokenizer.path=base_model//original/tokenizer.model \\\\\\n    checkpointer.checkpoint_dir=base_model/ \\\\\\n    checkpointer.output_dir=checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_2_20k-adapter-rev_1 \\\\\\n    dataset.data_files=data/resume_critique_model_2.jsonl \\\\\\n    metric_logger.group=resume-critique \\\\\\n    metric_logger.name=run-resume-critique-llama3_1_8b-tt_lora-model_2_20k-adapter-rev_1 \\\\\\n\\toutput_dir=checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_2_20k-adapter-rev_1 \\\\\\n\\tmetric_logger.log_dir=logs/\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%pybash\n",
    "tune run \\\n",
    "    --nproc_per_node {NUM_GPUS} \\\n",
    "    lora_finetune_distributed \\\n",
    "    --config {CONFIG_FILE_PATH} \\\n",
    "    tokenizer.path={TOKENIZER_PATH} \\\n",
    "    checkpointer.checkpoint_dir={BASE_MODEL_PATH} \\\n",
    "    checkpointer.output_dir={OUTPUT_MODEL_PATH} \\\n",
    "    dataset.data_files={TRAINING_DATA_PATH} \\\n",
    "    metric_logger.group={WANDB_GROUP_NAME} \\\n",
    "    metric_logger.name={RUN_WANDB_NAME} \\\n",
    "\toutput_dir={OUTPUT_MODEL_PATH} \\\n",
    "\tmetric_logger.log_dir={LOGS_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OUTPUT_MODEL_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m OUTPUT_MODEL_PATH\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OUTPUT_MODEL_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "OUTPUT_MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring files matching the following patterns: *.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 10 files: 100%|██████████| 10/10 [02:00<00:00, 12.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded model repo and wrote to the following locations:\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_2_rev_1/checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_2_20k-adapter-rev_1/hf_model_0001_0.pt\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_2_rev_1/checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_2_20k-adapter-rev_1/.gitattributes\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_2_rev_1/checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_2_20k-adapter-rev_1/adapter_0.pt\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_2_rev_1/checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_2_20k-adapter-rev_1/hf_model_0003_0.pt\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_2_rev_1/checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_2_20k-adapter-rev_1/adapter_model.bin\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_2_rev_1/checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_2_20k-adapter-rev_1/config.json\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_2_rev_1/checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_2_20k-adapter-rev_1/hf_model_0002_0.pt\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_2_rev_1/checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_2_20k-adapter-rev_1/hf_model_0004_0.pt\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_2_rev_1/checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_2_20k-adapter-rev_1/.cache\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_2_rev_1/checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_2_20k-adapter-rev_1/adapter_config.json\n",
      "/home/ubuntu/Development/interntal-fine-tune-foundry/resume-critique/model_2_rev_1/checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_2_20k-adapter-rev_1/recipe_state.pt\n"
     ]
    }
   ],
   "source": [
    "%%pybash\n",
    "tune download {FT_MODEL_HF_ID} --output-dir {OUTPUT_MODEL_PATH} --hf-token {HF_TOKEN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'config/llama_3_1_8b_lora_distributed_resume.yaml'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with torchrun...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1213 01:31:45.150000 14054 site-packages/torch/distributed/run.py:793] \n",
      "W1213 01:31:45.150000 14054 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "W1213 01:31:45.150000 14054 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1213 01:31:45.150000 14054 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "INFO:torchtune.utils._logging:Running LoRAFinetuneRecipeDistributed with resolved config:\n",
      "\n",
      "batch_size: 4\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  adapter_checkpoint: adapter_0.pt\n",
      "  checkpoint_dir: checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_2_20k-adapter-rev_1\n",
      "  checkpoint_files:\n",
      "  - hf_model_0001_0.pt\n",
      "  - hf_model_0002_0.pt\n",
      "  - hf_model_0003_0.pt\n",
      "  - hf_model_0004_0.pt\n",
      "  model_type: LLAMA3\n",
      "  output_dir: checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_2_20k-adapter-rev_1\n",
      "  recipe_checkpoint: recipe_state.pt\n",
      "compile: false\n",
      "dataset:\n",
      "  _component_: torchtune.datasets.alpaca_dataset\n",
      "  data_files: data/resume_critique_model_2.jsonl\n",
      "  packed: false\n",
      "  source: json\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 4\n",
      "gradient_accumulation_steps: 4\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
      "lr_scheduler:\n",
      "  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup\n",
      "  num_warmup_steps: 30\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.WandBLogger\n",
      "  group: resume-critique\n",
      "  log_dir: logs/\n",
      "  name: run-resume-critique-llama3_1_8b-tt_lora-model_2_20k-adapter-rev_1\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1.lora_llama3_1_8b\n",
      "  apply_lora_to_mlp: true\n",
      "  apply_lora_to_output: true\n",
      "  lora_alpha: 16\n",
      "  lora_attn_modules:\n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  lora_dropout: 0.05\n",
      "  lora_rank: 8\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 0.0003\n",
      "  weight_decay: 0.01\n",
      "output_dir: checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_2_20k-adapter-rev_1\n",
      "profiler:\n",
      "  _component_: torchtune.training.setup_torch_profiler\n",
      "  active_steps: 2\n",
      "  cpu: true\n",
      "  cuda: true\n",
      "  enabled: false\n",
      "  num_cycles: 1\n",
      "  output_dir: checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_2_20k-adapter-rev_1/profiling_outputs\n",
      "  profile_memory: false\n",
      "  record_shapes: true\n",
      "  wait_steps: 5\n",
      "  warmup_steps: 3\n",
      "  with_flops: false\n",
      "  with_stack: false\n",
      "resume_from_checkpoint: true\n",
      "save_adapter_weights_only: false\n",
      "seed: 42\n",
      "shuffle: true\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.llama3.llama3_tokenizer\n",
      "  max_seq_len: 4096\n",
      "  path: base_model//original/tokenizer.model\n",
      "\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0\n",
      "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: leonardo-mm (multimodalcompany). Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.0\n",
      "wandb: Run data is saved locally in logs/wandb/run-20241213_013149-foqsxs1e\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run run-resume-critique-llama3_1_8b-tt_lora-model_2_20k-adapter-rev_1\n",
      "wandb: ⭐️ View project at https://wandb.ai/multimodalcompany/torchtune\n",
      "wandb: 🚀 View run at https://wandb.ai/multimodalcompany/torchtune/runs/foqsxs1e\n",
      "INFO:torchtune.utils._logging:Logging checkpoint/multimodalai/resume-critique-llama3_1_8b-tt_lora-model_2_20k-adapter-rev_1/torchtune_config.yaml to W&B under Files\n",
      "INFO:torchtune.utils._logging:FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n",
      "INFO:torchtune.utils._logging:Instantiating model and loading checkpoint took 5.03 secs\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 4.79 GiB\n",
      "\tGPU peak memory reserved: 4.91 GiB\n",
      "\tGPU peak memory active: 4.79 GiB\n",
      "INFO:torchtune.utils._logging:Optimizer is initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "Generating train split: 19365 examples [00:01, 10051.23 examples/s]\n",
      "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
      "INFO:torchtune.utils._logging:Learning rate scheduler is initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n"
     ]
    }
   ],
   "source": [
    "%%pybash\n",
    "tune run \\\n",
    "    --nproc_per_node {NUM_GPUS} \\\n",
    "    lora_finetune_distributed \\\n",
    "    --config {CONFIG_FILE_PATH} \\\n",
    "    tokenizer.path={TOKENIZER_PATH} \\\n",
    "    checkpointer.checkpoint_dir={OUTPUT_MODEL_PATH} \\\n",
    "    checkpointer.output_dir={OUTPUT_MODEL_PATH} \\\n",
    "    dataset.data_files={TRAINING_DATA_PATH} \\\n",
    "    metric_logger.group={WANDB_GROUP_NAME} \\\n",
    "    metric_logger.name={RUN_WANDB_NAME} \\\n",
    "\toutput_dir={OUTPUT_MODEL_PATH} \\\n",
    "\tmetric_logger.log_dir={LOGS_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
